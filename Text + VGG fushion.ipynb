{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import torch\n",
    "import bcolz\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import utils; imp.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preconvoluted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/foodlovers/FoodVisor/data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x))\n",
    "         for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/foodlovers/FoodVisor/data/images/train/apple_pie/apple_pie_0.jpg', 0),\n",
       " ('/home/foodlovers/FoodVisor/data/images/train/apple_pie/apple_pie_100.jpg',\n",
       "  0),\n",
       " ('/home/foodlovers/FoodVisor/data/images/train/apple_pie/apple_pie_101.jpg',\n",
       "  0),\n",
       " ('/home/foodlovers/FoodVisor/data/images/train/apple_pie/apple_pie_102.jpg',\n",
       "  0),\n",
       " ('/home/foodlovers/FoodVisor/data/images/train/apple_pie/apple_pie_104.jpg',\n",
       "  0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets['train'].imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat_train = load_array('/home/foodlovers/vgg16/conv_feat_train.bc')\n",
    "labels_train = load_array('/home/foodlovers/vgg16/labels_train.bc')\n",
    "conv_feat_val = load_array('/home/foodlovers/vgg16/conv_feat_val.bc')\n",
    "labels_val = load_array('/home/foodlovers/vgg16/labels_val.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67988, 512, 7, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets['train'].imgs) == len(conv_feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels_train[:10])\n",
    "print([x[1] for x in dsets['train'].imgs[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True !\n"
     ]
    }
   ],
   "source": [
    "for i,label in enumerate(labels_train) :\n",
    "    if label != dsets['train'].imgs[i][1] :\n",
    "        print(\"Wrong !!!\")\n",
    "for i,label in enumerate(labels_val) :\n",
    "    if label != dsets['test'].imgs[i][1] :\n",
    "        print(\"Wrong !!!\")\n",
    "print(\"True !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corresponding text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed textes in Food_Text_Classifier.ipynb\n",
    "text_dir = '/home/foodlovers/FoodVisor/data/processed_texts_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_pie', 'apple_pie_100.jpg']\n",
      "apple_pie/apple_pie_100.txt\n",
      "/home/foodlovers/FoodVisor/data/processed_texts_txt/apple_pie/apple_pie_100.txt\n",
      "My account My newslett My groceri list My save custom care log My box submit log subscrib digit edit give gift cookbook subscrib best BY cours appet breakfast dessert dinner side cours BY beef chicken strawberri tomato watermelon BY style grill ahead potluck quick slow cooker style healthi budget video partner advanc see submit contest save best deal subscrib today holiday holiday juli th labor day celebr summer entertain birthday holiday celebr fourth OF juli flagshap grill classic american pie red white blue dessert juli th commun commun favorit forum group commun facebook pinterest tumblr google twitter popular pin check pinterest follow school school Us find show school meet staff faq school find show attend show fun uniqu night learn skill memori 'll last lifetim find show cookbook across america discov countri best flavor shop cookbook across america ecookbook cookbook subscrib tri america 'S magazin real best famili favorit everi issu tast save best deal subscrib today subscrib newslett issu digit edit give gift custom care subscrib renew glaze appl pie squar glaze appl pie squar photo tast glaze appl pie squar read review realli good chang appl pie sinc glaze littl bit sweeter dough firm enough eat hand best appl season dian turner brunswick ohio inbox day free day newslett inbox day free day newslett total prep hour bake min cool box print friend total prep hour bake min cool allpurpos flour teaspoon salt cold butter cube egg separ tablespoon milk crush cornflak thinli slice peel tart appl medium plu tablespoon sugar divid teaspoon ground cinnamon divid teaspoon ground nutmeg glaze confection sugar teaspoon vanilla extract tablespoon milk My save groceri list nutrit fact squar equal calori fat satur fat mg cholesterol mg sodium carbohydr fiber protein direct larg bowl combin flour salt cut butter mixtur resembl coars crumb measur combin egg yolk enough milk measur gradual flour mixtur toss fork dough form ball preheat oven divid dough half roll portion thin xin rectangl transfer bottom ungreas xx bake pan sprinkl cornflak larg bowl combin appl sugar teaspoon cinnamon nutmeg toss coat spoon crust roll remain dough thin xin rectangl place appl fill beat egg white brush pastri combin remain sugar cinnamon sprinkl bake golden brown glaze combin confection sugar vanilla enough milk achiev drizzl consist drizzl warm pastri cool complet wire rack cut squar yield dozen origin publish glaze appl pie squar tast februarymarch nutrit fact squar equal calori fat satur fat mg cholesterol mg sodium carbohydr fiber protein print box friend review glaze appl pie squar averag rate rate distribut star star star star star MY review pleas log join rate review log review click star rate ani chang rate review appear origin review submit review edit review updat review cancel sort By newest first oldest first highest rate lowest rate MY review review may yum MY review yahoocom review nov made sever first exactli written except pan My husband sinc made less sugar still veri veri good MY review pooki mama review nov mani pick 'm glad fresh pick appl omit cornflak extra flour sweet rich flavor held veri nice bar got rave review gone quickli crust tender flakey wow MY review review sep got exact aunt year ago except call danish appl bar 're delici MY review review sep year onli coupl tbsp sugar becaus appl tart usual cinnamon tast instead vanilla glaze real almond realli someth alway cornflak crispix special etc sometim sprinkl bottom mix appl cinnamon mix brought work never ani left coffe break staff would sneak would sure view review similar sausag appl squar sour cream appl squar appl snack squar appl walnut squar appl harvest squar mock appl pie squar appl pastri squar applesauc gelatin squar pear dessert squar appl walnut bar cognac date squar creami cashew browni collect tast magazin appl dessert appl pie appl pie crust appl pie appl pie fresh appl appl bar comfort dessert comfort dessert father day fruit pie homemad pie crust low fat dessert low fat pie bake pie crust pie crust butter pie potluck dessert potluck sweet finger tast magazin dessert tast magazin advertis us advertis sign Up newslett popular month deliv inbox sign UP invalid contest promot follow Us advertis us advertis advertis us advertis print ipad kindl nook holiday school commun cookbook magazin Us contest rss magazin custom servic Us log log help press room advertis site map term condit privaci sign UP newslett sign UP invalid follow US Us contest rss magazin custom servic Us log log help privaci press room advertis site map term condit rda enthusiast brand llc select usernam field requir usernam usernam incorrect pleas select usernam appear public area site commun request usernam enter valid usernam usernam alreadi exist usernam contain space special charact password pleas enter password forgot password rememb Me log cancel log cancel join member whi save favorit free person box thought made sign free newslett join\n"
     ]
    }
   ],
   "source": [
    "file_path = dsets['train'].imgs[1][0]\n",
    "print(file_path.split(\"/\")[-2:])\n",
    "print(\"/\".join(file_path.split(\"/\")[-2:])[:-3]+'txt')\n",
    "path_text = text_dir+'/'+\"/\".join(file_path.split(\"/\")[-2:])[:-3]+'txt'\n",
    "print(path_text)\n",
    "text_file = open(path_text, \"r\")\n",
    "text = text_file.read()\n",
    "text_file.close()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908  images don't have text file\n"
     ]
    }
   ],
   "source": [
    "text_train = []\n",
    "text_val = []\n",
    "# a lot of file don't contain any text\n",
    "valid_conv_feat_train = []\n",
    "valid_labels_train = []\n",
    "valid_conv_feat_val = []\n",
    "valid_labels_val = []\n",
    "\n",
    "unvalid_count = 0\n",
    "for i in range(len(conv_feat_train)) :\n",
    "    try :\n",
    "        file_path = dsets['train'].imgs[i][0]\n",
    "        path_text = text_dir+'/'+\"/\".join(file_path.split(\"/\")[-2:])[:-3]+'txt'\n",
    "        text_file = open(path_text, \"r\")\n",
    "        text = text_file.read()\n",
    "        text_file.close()\n",
    "        text_train.append(text)\n",
    "        valid_conv_feat_train.append(conv_feat_train[i])\n",
    "        valid_labels_train.append(labels_train[i])\n",
    "    except FileNotFoundError :\n",
    "        # print(file_path)\n",
    "        unvalid_count += 1\n",
    "    \n",
    "for i in range(len(conv_feat_val)) :\n",
    "    try :\n",
    "        file_path = dsets['test'].imgs[i][0]\n",
    "        path_text = text_dir+'/'+\"/\".join(file_path.split(\"/\")[-2:])[:-3]+'txt'\n",
    "        text_file = open(path_text, \"r\")\n",
    "        text = text_file.read()\n",
    "        text_file.close()\n",
    "        text_val.append(text)\n",
    "        valid_conv_feat_val.append(conv_feat_val[i])\n",
    "        valid_labels_val.append(labels_val[i])\n",
    "    except FileNotFoundError :\n",
    "        # print(file_path)\n",
    "        unvalid_count += 1\n",
    "        \n",
    "print(unvalid_count,\" images don't have text file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 65101 test: 21695\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",len(text_train),\"test:\",len(text_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform text file into TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(binary=True)\n",
    "text_feat_train = tfidf.fit_transform(text_train)\n",
    "#.astype('float16')\n",
    "text_feat_val = tfidf.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65101, 60033)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_conv_feat_train = np.asarray(valid_conv_feat_train).reshape(len(valid_conv_feat_train),-1)\n",
    "valid_conv_feat_val = np.asarray(valid_conv_feat_val).reshape(len(valid_conv_feat_val),-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test image features with linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 36s, sys: 3.93 s, total: 21min 40s\n",
      "Wall time: 21min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC()\n",
    "clf.fit(valid_conv_feat_train, valid_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.3752477529384651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(valid_conv_feat_val)\n",
    "accuracy = accuracy_score(valid_labels_val, y_pred)\n",
    "print(\"Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build my model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count les nombres de parametres ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: (65101, 60033) Image features shape: (65101, 25088)\n"
     ]
    }
   ],
   "source": [
    "print(\"Text features shape:\",text_feat_train.shape,\"Image features shape:\",valid_conv_feat_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train textual features with a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(60033, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 101),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = myNet()\n",
    "\n",
    "if use_gpu:\n",
    "    net1 = net1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen_1(data,labels,batch_size=64,shuffle=True):\n",
    "    labels = np.array(labels)\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(len(labels))\n",
    "        data = data[index]\n",
    "        labels = labels[index]\n",
    "    for idx in range(0,len(labels),batch_size):\n",
    "        if type(data).__module__ == 'numpy' :\n",
    "            yield(data[idx:idx+batch_size],labels[idx:idx+batch_size],\n",
    "             int(len(labels) / batch_size) + (len(labels) % batch_size > 0))\n",
    "        else :\n",
    "            yield(data[idx:idx+batch_size].toarray(),labels[idx:idx+batch_size],\n",
    "             int(len(labels) / batch_size) + (len(labels) % batch_size > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(model, criterion, batch_size = 64,\n",
    "                 train_data = None, train_labels = None,\n",
    "                 test_data = None, test_labels = None,\n",
    "                  optimizer = None,\n",
    "                 epochs = 1,train = True, validate = False,\n",
    "                shuffle = True) :\n",
    "    \n",
    "    if train == True :\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "    for epoch in range(epochs) :\n",
    "        if train == True :\n",
    "            #=========================TRAINING=================================#\n",
    "            start_time_epoch = time.time()\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "            print(\"Epoch:\", epoch,\"/\",epochs-1,\"===============================================\")\n",
    "        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen_1(data=train_data,labels=train_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "        \n",
    "                inputs,classes,batch_num = data\n",
    "\n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "                    \n",
    "                inputs = inputs.float()\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                    \n",
    "                # calulate outputs and losses\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)       \n",
    "\n",
    "                # autograd\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_loss = loss.data.item()\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                batch_corrects = torch.sum(preds == classes.data)\n",
    "                \n",
    "                running_loss += batch_loss\n",
    "                running_corrects += batch_corrects\n",
    "\n",
    "                print('Batch {:d}/{:d} - Loss: {:.4f} Acc: {:.4f} - Time : {:.2f}s'.format(i+1,batch_num,\n",
    "                             batch_loss/len(classes), float(batch_corrects)/len(classes), time.time() - start_time),end='\\r')\n",
    "\n",
    "            epoch_loss = running_loss / len(train_labels)\n",
    "            epoch_acc = running_corrects.data.item() / len(train_labels)\n",
    "            #\n",
    "            \n",
    "            loss_history.append(epoch_loss)\n",
    "            acc_history.append(epoch_acc)\n",
    "            \n",
    "            print('Epoch {:d} completed in {:.2f} seconds ! Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch , time.time() - start_time_epoch, epoch_loss, epoch_acc))\n",
    "            \n",
    "        if validate == True :\n",
    "            #=========================VALIDATING=================================#\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen_1(data=test_data,labels=test_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "                \n",
    "                inputs,classes,batch_num = data\n",
    "                \n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "                    \n",
    "                inputs = inputs.float()\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "                    \n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)        \n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "\n",
    "                # statistics\n",
    "\n",
    "                val_loss += loss.data.item()\n",
    "                val_corrects += torch.sum(preds == classes.data)\n",
    "                \n",
    "                print('Validating batch {:d}/{:d} - {:.2f}s ...'.format(i+1,batch_num\n",
    "                                                                , time.time() - start_time), end=\"\\r\")\n",
    "\n",
    "            val_epoch_loss = val_loss / len(test_labels)\n",
    "            val_epoch_acc = val_corrects.data.item() / len(test_labels)\n",
    "            # \n",
    "\n",
    "            print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(\n",
    "                             val_epoch_loss,val_epoch_acc))\n",
    "            \n",
    "            if train == False :\n",
    "                return\n",
    "            else :\n",
    "                val_loss_history.append(val_epoch_loss)\n",
    "                val_acc_history.append(val_epoch_acc)\n",
    "    \n",
    "    if train == False :\n",
    "        return 'On fait rien!'\n",
    "    elif validate == False :\n",
    "        return loss_history, acc_history\n",
    "    else :\n",
    "        return loss_history, acc_history,val_loss_history,val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "# optimizer = torch.optim.SGD(net1.parameters(),lr = lr,momentum = 0.9)\n",
    "optimizer = torch.optim.Adam(net1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 9 ===============================================\n",
      "Epoch 0 completed in 70.42 seconds ! Loss: 0.0309 Acc: 0.5306\n",
      "Val Loss: 0.0157 Val Acc: 0.7637 ...\n",
      "Epoch: 1 / 9 ===============================================\n",
      "Epoch 1 completed in 70.95 seconds ! Loss: 0.0096 Acc: 0.8518\n",
      "Val Loss: 0.0138 Val Acc: 0.7946 ...\n",
      "Epoch: 2 / 9 ===============================================\n",
      "Epoch 2 completed in 71.01 seconds ! Loss: 0.0054 Acc: 0.9198\n",
      "Val Loss: 0.0138 Val Acc: 0.8025 ...\n",
      "Epoch: 3 / 9 ===============================================\n",
      "Epoch 3 completed in 71.06 seconds ! Loss: 0.0042 Acc: 0.9376\n",
      "Val Loss: 0.0143 Val Acc: 0.8022 ...\n",
      "Epoch: 4 / 9 ===============================================\n",
      "Epoch 4 completed in 70.94 seconds ! Loss: 0.0036 Acc: 0.9453\n",
      "Val Loss: 0.0149 Val Acc: 0.8018 ...\n",
      "Epoch: 5 / 9 ===============================================\n",
      "Epoch 5 completed in 70.99 seconds ! Loss: 0.0033 Acc: 0.9496\n",
      "Val Loss: 0.0149 Val Acc: 0.8057 ...\n",
      "Epoch: 6 / 9 ===============================================\n",
      "Epoch 6 completed in 71.07 seconds ! Loss: 0.0030 Acc: 0.9520\n",
      "Val Loss: 0.0151 Val Acc: 0.8070 ...\n",
      "Epoch: 7 / 9 ===============================================\n",
      "Batch 84/1018 - Loss: 0.0031 Acc: 0.9531 - Time : 0.06s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-80c116f28094>\u001b[0m in \u001b[0;36mtrain_model_1\u001b[0;34m(model, criterion, batch_size, train_data, train_labels, test_data, test_labels, optimizer, epochs, train, validate, shuffle)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mbatch_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train_model_1(net1,\n",
    "                train_data=text_feat_train ,train_labels=valid_labels_train , \n",
    "                test_data = text_feat_val , test_labels = valid_labels_val ,\n",
    "                epochs=5, batch_size = 32,\n",
    "                optimizer=optimizer,criterion=criterion ,\n",
    "                train=True, validate = True,\n",
    "                shuffle=True )\n",
    "print(\"DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0161 Val Acc: 0.8069 ...\n"
     ]
    }
   ],
   "source": [
    "history = train_model_1(net1,\n",
    "                train_data=text_feat_train ,train_labels=valid_labels_train , \n",
    "                test_data = text_feat_val , test_labels = valid_labels_val ,\n",
    "                epochs=10, batch_size = 32,\n",
    "                optimizer=optimizer,criterion=criterion ,\n",
    "                train=False, validate = True,\n",
    "                shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.classifier._modules['6'] = nn.Linear(4096, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model_vgg = model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_vgg.classifier.parameters(),lr = lr,momentum = 0.9)\n",
    "# optimizer = torch.optim.Adam(model_vgg.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Dropout(p=0.5)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace)\n",
      "  (5): Dropout(p=0.5)\n",
      "  (6): Linear(in_features=4096, out_features=101, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_vgg.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_vgg.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 9 ===============================================\n",
      "Epoch 0 completed in 61.91 seconds ! Loss: 0.0504 Acc: 0.2560\n",
      "Val Loss: 0.0420 Val Acc: 0.3759 ...\n",
      "Epoch: 1 / 9 ===============================================\n",
      "Epoch 1 completed in 61.94 seconds ! Loss: 0.0438 Acc: 0.3401\n",
      "Val Loss: 0.0406 Val Acc: 0.4154 ...\n",
      "Epoch: 2 / 9 ===============================================\n",
      "Epoch 2 completed in 62.06 seconds ! Loss: 0.0390 Acc: 0.4026\n",
      "Val Loss: 0.0403 Val Acc: 0.4244 ...\n",
      "Epoch: 3 / 9 ===============================================\n",
      "Epoch 3 completed in 62.72 seconds ! Loss: 0.0347 Acc: 0.4588\n",
      "Val Loss: 0.0400 Val Acc: 0.4315 ...\n",
      "Epoch: 4 / 9 ===============================================\n",
      "Epoch 4 completed in 62.13 seconds ! Loss: 0.0310 Acc: 0.5140\n",
      "Val Loss: 0.0393 Val Acc: 0.4341 ...\n",
      "Epoch: 5 / 9 ===============================================\n",
      "Epoch 5 completed in 62.15 seconds ! Loss: 0.0278 Acc: 0.5620\n",
      "Val Loss: 0.0403 Val Acc: 0.4319 ...\n",
      "Epoch: 6 / 9 ===============================================\n",
      "Epoch 6 completed in 62.11 seconds ! Loss: 0.0252 Acc: 0.5991\n",
      "Val Loss: 0.0406 Val Acc: 0.4268 ...\n",
      "Epoch: 7 / 9 ===============================================\n",
      "Epoch 7 completed in 62.74 seconds ! Loss: 0.0236 Acc: 0.6256\n",
      "Val Loss: 0.0407 Val Acc: 0.4278 ...\n",
      "Epoch: 8 / 9 ===============================================\n",
      "Batch 232/1018 - Loss: 0.0182 Acc: 0.6406 - Time : 0.06s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-80c116f28094>\u001b[0m in \u001b[0;36mtrain_model_1\u001b[0;34m(model, criterion, batch_size, train_data, train_labels, test_data, test_labels, optimizer, epochs, train, validate, shuffle)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mbatch_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train_model_1(model_vgg.classifier,\n",
    "                train_data=valid_conv_feat_train  ,train_labels=valid_labels_train , \n",
    "                test_data = valid_conv_feat_val  , test_labels = valid_labels_val ,\n",
    "                epochs=5, batch_size = 32,\n",
    "                optimizer=optimizer,criterion=criterion ,\n",
    "                train=True, validate = True,\n",
    "                shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0404 Val Acc: 0.4382 ...\n"
     ]
    }
   ],
   "source": [
    "train_model_1(model_vgg.classifier,\n",
    "                train_data=valid_conv_feat_train  ,train_labels=valid_labels_train , \n",
    "                test_data = valid_conv_feat_val  , test_labels = valid_labels_val ,\n",
    "                epochs=10, batch_size = 32,\n",
    "                optimizer=optimizer,criterion=criterion ,\n",
    "                train=False, validate = True,\n",
    "                shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myFusionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myFusionNet, self).__init__()\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(60033, 512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 128),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(128, 101),\n",
    "#         ) \n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Linear(25088, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(4096, 101),\n",
    "#         ) \n",
    "        self.ratio = torch.tensor(0.8, requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape (?,60033+25088)\n",
    "        x1 = x[:,:60033]\n",
    "        x2 = x[:,-25088:]\n",
    "        #x1 = self.fc1(x1)\n",
    "        #x2 = self.fc2(x2)\n",
    "        x1 = net1(x1)\n",
    "        x2 = model_vgg.classifier(x2)\n",
    "        out = self.ratio*x1 + (1-self.ratio)*x2\n",
    "        # out = torch.cat((x1,x2),1)\n",
    "        # out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myFusionNet()\n"
     ]
    }
   ],
   "source": [
    "mynet = myFusionNet()\n",
    "print(mynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    mynet = mynet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(list([mynet.ratio]),lr = lr,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define datagen + train_model for data fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data1,data2,labels,batch_size=32,shuffle=True):\n",
    "    \"\"\"\n",
    "    data1 expected to be text_feat of type csr_matrix\n",
    "    data2 expected to be conv_feat of type ndarray\n",
    "    \"\"\"\n",
    "    labels = np.array(labels)\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(len(labels))\n",
    "        data1 = data1[index]\n",
    "        data2 = data2[index]\n",
    "        labels = labels[index]\n",
    "    for idx in range(0,len(labels),batch_size):\n",
    "        yield(np.concatenate((data1[idx:idx+batch_size].toarray(), data2[idx:idx+batch_size]), axis=1)\n",
    "              ,labels[idx:idx+batch_size],\n",
    "             int(len(labels) / batch_size) + (len(labels) % batch_size > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(model, criterion,\n",
    "                 train_data1 = None,train_data2=None, train_labels = None,\n",
    "                 test_data1 = None,test_data2=None, test_labels = None,\n",
    "                  optimizer = None,\n",
    "                 epochs = 1,train = True, validate = False,\n",
    "                shuffle = True) :\n",
    "    \n",
    "    if train == True :\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "    for epoch in range(epochs) :\n",
    "        if train == True :\n",
    "            #=========================TRAINING=================================#\n",
    "            start_time_epoch = time.time()\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "            print(\"Epoch:\", epoch,\"/\",epochs-1,\"===============================================\")\n",
    "        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen(data1=train_data1,data2=train_data2,labels=train_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "        \n",
    "                inputs,classes,batch_num = data\n",
    "\n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "                    \n",
    "                inputs = inputs.float()\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                    \n",
    "                # calulate outputs and losses\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)       \n",
    "\n",
    "                # autograd\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_loss = loss.data.item()\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                batch_corrects = torch.sum(preds == classes.data)\n",
    "                \n",
    "                running_loss += batch_loss\n",
    "                running_corrects += batch_corrects\n",
    "\n",
    "                print('Batch {:d}/{:d} - Loss: {:.4f} Acc: {:.4f} - Time : {:.2f}s'.format(i+1,batch_num,\n",
    "                             batch_loss/len(classes), float(batch_corrects)/len(classes), time.time() - start_time),end='\\r')\n",
    "\n",
    "            epoch_loss = running_loss / len(train_labels)\n",
    "            epoch_acc = running_corrects.data.item() / len(train_labels)\n",
    "            #\n",
    "            \n",
    "            loss_history.append(epoch_loss)\n",
    "            acc_history.append(epoch_acc)\n",
    "            \n",
    "            print('Epoch {:d} completed in {:.2f} seconds ! Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch , time.time() - start_time_epoch, epoch_loss, epoch_acc))\n",
    "            \n",
    "        if validate == True :\n",
    "            #=========================VALIDATING=================================#\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen(data1=test_data1,data2=test_data2,labels=test_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "                \n",
    "                inputs,classes,batch_num = data\n",
    "                \n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "                    \n",
    "                inputs = inputs.float()\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "                    \n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)        \n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "\n",
    "                # statistics\n",
    "\n",
    "                val_loss += loss.data.item()\n",
    "                val_corrects += torch.sum(preds == classes.data)\n",
    "                \n",
    "                print('Validating batch {:d}/{:d} - {:.2f}s ...'.format(i+1,batch_num\n",
    "                                                                , time.time() - start_time), end=\"\\r\")\n",
    "\n",
    "            val_epoch_loss = val_loss / len(test_labels)\n",
    "            val_epoch_acc = val_corrects.data.item() / len(test_labels)\n",
    "            # \n",
    "\n",
    "            print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(\n",
    "                             val_epoch_loss,val_epoch_acc))\n",
    "            \n",
    "            if train == False :\n",
    "                return\n",
    "            else :\n",
    "                val_loss_history.append(val_epoch_loss)\n",
    "                val_acc_history.append(val_epoch_acc)\n",
    "    \n",
    "    if train == False :\n",
    "        return 'On fait rien!'\n",
    "    elif validate == False :\n",
    "        return loss_history, acc_history\n",
    "    else :\n",
    "        return loss_history, acc_history,val_loss_history,val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 9 ===============================================\n",
      "Epoch 0 completed in 111.89 seconds ! Loss: 0.0015 Acc: 0.9941\n",
      "Val Loss: 0.0175 Val Acc: 0.8642 ...\n",
      "Epoch: 1 / 9 ===============================================\n",
      "Epoch 1 completed in 112.06 seconds ! Loss: 0.0015 Acc: 0.9942\n",
      "Val Loss: 0.0178 Val Acc: 0.8626 ...\n",
      "Epoch: 2 / 9 ===============================================\n",
      "Batch 225/2035 - Loss: 0.0002 Acc: 1.0000 - Time : 0.05s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-9541385cef12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 shuffle = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-2335154851e5>\u001b[0m in \u001b[0;36mtrain_model_2\u001b[0;34m(model, criterion, train_data1, train_data2, train_labels, test_data1, test_data2, test_labels, optimizer, epochs, train, validate, shuffle)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mbatch_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model_2(model=mynet,criterion=criterion,\n",
    "              train_data1 = text_feat_train,train_data2=valid_conv_feat_train, train_labels = valid_labels_train,\n",
    "                 test_data1 = text_feat_val,test_data2= valid_conv_feat_val, test_labels = valid_labels_val,\n",
    "                  optimizer = optimizer,\n",
    "                 epochs = 10,train = True, validate = True,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5083, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0176 Val Acc: 0.8637 ...\n"
     ]
    }
   ],
   "source": [
    "history = train_model_2(model=mynet,criterion=criterion,\n",
    "              train_data1 = text_feat_train,train_data2=valid_conv_feat_train, train_labels = valid_labels_train,\n",
    "                 test_data1 = text_feat_val,test_data2= valid_conv_feat_val, test_labels = valid_labels_val,\n",
    "                  optimizer = optimizer,\n",
    "                 epochs = 10,train = False, validate = True,\n",
    "                shuffle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
