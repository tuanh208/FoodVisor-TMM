{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import torch\n",
    "import bcolz\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import utils; imp.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print('Using gpu: %s ' % use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'D:\\\\Tu Beo\\\\Education\\\\FoodVisor\\\\data\\\\UPMC_Food101\\\\images'\n",
    "data_dir = '/home/foodlovers/FoodVisor/data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "         for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets['train'].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsets['train'].imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_classes = dsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 67988, 'test': 22716}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'test']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=64,\n",
    "                                               shuffle=False\n",
    "                                               # shuffle_valtrain(x)\n",
    "                                               , num_workers=6)\n",
    "                for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = torch.utils.data.DataLoader(dsets['test'], batch_size=5, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for data in dataset_valid:\n",
    "    if count == 0:\n",
    "        inputs_try,labels_try = data\n",
    "    else:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs_try)\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in labels_try])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dset_loaders['train']))\n",
    "\n",
    "n_images = 8\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of validation data\n",
    "inputs, classes = next(iter(dset_loaders['test']))\n",
    "\n",
    "n_images = 8\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_try , labels_try = var_cgpu(inputs_try,use_gpu),var_cgpu(labels_try,use_gpu)\n",
    "\n",
    "if use_gpu:\n",
    "    model_vgg = model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try = model_vgg(inputs_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the last layer and setting the gradient false to all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=101, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model_vgg.classifier._modules['6'] = nn.Linear(4096, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_vgg.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model_vgg = model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating preconvoluted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preconvfeat(dataset):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "    count = 0\n",
    "    for data in dataset:\n",
    "        count += 1\n",
    "        print(count,\"/\",len(dataset),end='\\r')\n",
    "        \n",
    "        inputs,labels = data\n",
    "        if use_gpu:\n",
    "            inputs , labels = Variable(inputs.cuda()),Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs , labels = Variable(inputs),Variable(labels)\n",
    "        \n",
    "        x = model_vgg.features(inputs)\n",
    "        conv_features.extend(x.data.cpu().numpy())\n",
    "        labels_list.extend(labels.data.cpu().numpy())\n",
    "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
    "    return (conv_features,labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 / 1063\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 6 bytes but only got 0. Skipping tag 271\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 7 bytes but only got 0. Skipping tag 272\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:780: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 45s, sys: 4min 5s, total: 11min 51s\n",
      "Wall time: 11min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_feat_train,labels_train = preconvfeat(dset_loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 1min 22s, total: 3min 58s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_feat_val,labels_val = preconvfeat(dset_loaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array('/home/foodlovers/vgg16/conv_feat_train.bc',conv_feat_train)\n",
    "save_array('/home/foodlovers/vgg16/labels_train.bc',labels_train)\n",
    "save_array('/home/foodlovers/vgg16/conv_feat_val.bc',conv_feat_val)\n",
    "save_array('/home/foodlovers/vgg16/labels_val.bc',labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67988, 512, 7, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat_train = load_array('/home/foodlovers/vgg16/conv_feat_train.bc')\n",
    "labels_train = load_array('/home/foodlovers/vgg16/labels_train.bc')\n",
    "conv_feat_val = load_array('/home/foodlovers/vgg16/conv_feat_val.bc')\n",
    "labels_val = load_array('/home/foodlovers/vgg16/labels_val.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training fully connected module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_vgg.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optimizer_vgg = torch.optim.SGD(model_vgg.classifier.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_vgg, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(conv_feat,labels,batch_size=64,shuffle=True):\n",
    "    labels = np.array(labels)\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(len(labels))\n",
    "        conv_feat = conv_feat[index]\n",
    "        labels = labels[index]\n",
    "    for idx in range(0,len(labels),batch_size):\n",
    "        yield(conv_feat[idx:idx+batch_size],labels[idx:idx+batch_size],int(len(labels) / batch_size) + (len(labels) % batch_size > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(model, criterion,\n",
    "                 train_data = None, train_labels = None,\n",
    "                 test_data = None, test_labels = None,\n",
    "                  optimizer = None,\n",
    "                 epochs = 1,train = True, validate = False,\n",
    "                shuffle = True) :\n",
    "    \n",
    "    if train == True :\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "    for epoch in range(epochs) :\n",
    "        \n",
    "        if train == True :\n",
    "            #=========================TRAINING=================================#\n",
    "            start_time_epoch = time.time()\n",
    "            \n",
    "            # scheduler.step()\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "            print(\"Epoch:\", epoch,\"/\",epochs-1,\"===============================================\")\n",
    "        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen(conv_feat=train_data,labels=train_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "        \n",
    "                inputs,classes,batch_num = data\n",
    "\n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                    \n",
    "                # calulate outputs and losses\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)       \n",
    "\n",
    "                # autograd\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_loss = loss.data.item()\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                batch_corrects = torch.sum(preds == classes.data)\n",
    "                \n",
    "                running_loss += batch_loss\n",
    "                running_corrects += batch_corrects\n",
    "\n",
    "                print('Batch {:d}/{:d} - Loss: {:.4f} Acc: {:.4f} - Time : {:.2f}s'.format(i+1,batch_num,\n",
    "                             batch_loss/len(classes), float(batch_corrects)/len(classes), time.time() - start_time),end='\\r')\n",
    "\n",
    "            epoch_loss = running_loss / len(train_labels)\n",
    "            epoch_acc = running_corrects.data.item() / len(train_labels)\n",
    "            #\n",
    "            \n",
    "            loss_history.append(epoch_loss)\n",
    "            acc_history.append(epoch_acc)\n",
    "            \n",
    "            print('Epoch {:d} completed in {:.2f} seconds ! Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch , time.time() - start_time_epoch, epoch_loss, epoch_acc))\n",
    "            \n",
    "        if validate == True :\n",
    "            #=========================VALIDATING=================================#\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0.0\n",
    "            \n",
    "            batches = data_gen(conv_feat=test_data,labels=test_labels,shuffle=shuffle)\n",
    "            \n",
    "            #batch_num = len(list(batches))\n",
    "\n",
    "            for i,data in enumerate(batches) :\n",
    "                start_time = time.time()\n",
    "                \n",
    "                inputs,classes,batch_num = data\n",
    "                \n",
    "                if  isinstance(inputs, (list, np.ndarray)) :\n",
    "                    inputs , classes = torch.from_numpy(inputs), torch.from_numpy(classes)\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs , classes = inputs.cuda(), classes.cuda()\n",
    "                    \n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)        \n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "\n",
    "                # statistics\n",
    "\n",
    "                val_loss += loss.data.item()\n",
    "                val_corrects += torch.sum(preds == classes.data)\n",
    "                \n",
    "                print('Validating batch {:d}/{:d} - {:.2f}s ...'.format(i+1,batch_num\n",
    "                                                                , time.time() - start_time), end=\"\\r\")\n",
    "\n",
    "            val_epoch_loss = val_loss / len(test_labels)\n",
    "            val_epoch_acc = val_corrects.data.item() / len(test_labels)\n",
    "            # \n",
    "\n",
    "            print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(\n",
    "                             val_epoch_loss,val_epoch_acc))\n",
    "            \n",
    "            if train == False :\n",
    "                return\n",
    "            else :\n",
    "                val_loss_history.append(val_epoch_loss)\n",
    "                val_acc_history.append(val_epoch_acc)\n",
    "    \n",
    "    if train == False :\n",
    "        return 'On fait rien!'\n",
    "    elif validate == False :\n",
    "        return loss_history, acc_history\n",
    "    else :\n",
    "        return loss_history, acc_history,val_loss_history,val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 19 ===============================================\n",
      "Epoch 0 completed in 50.84 seconds ! Loss: 0.0491 Acc: 0.2743\n",
      "Val Loss: 0.0376 Val Acc: 0.4275 ...\n",
      "Epoch: 1 / 19 ===============================================\n",
      "Epoch 1 completed in 50.96 seconds ! Loss: 0.0408 Acc: 0.3769\n",
      "Val Loss: 0.0355 Val Acc: 0.4562 ...\n",
      "Epoch: 2 / 19 ===============================================\n",
      "Epoch 2 completed in 50.71 seconds ! Loss: 0.0373 Acc: 0.4236\n",
      "Val Loss: 0.0340 Val Acc: 0.4776 ...\n",
      "Epoch: 3 / 19 ===============================================\n",
      "Epoch 3 completed in 50.74 seconds ! Loss: 0.0344 Acc: 0.4588\n",
      "Val Loss: 0.0331 Val Acc: 0.4961 ...\n",
      "Epoch: 4 / 19 ===============================================\n",
      "Epoch 4 completed in 50.77 seconds ! Loss: 0.0316 Acc: 0.4989\n",
      "Val Loss: 0.0326 Val Acc: 0.5034 ...\n",
      "Epoch: 5 / 19 ===============================================\n",
      "Epoch 5 completed in 50.88 seconds ! Loss: 0.0281 Acc: 0.5491\n",
      "Val Loss: 0.0322 Val Acc: 0.5088 ...\n",
      "Epoch: 6 / 19 ===============================================\n",
      "Epoch 6 completed in 50.79 seconds ! Loss: 0.0275 Acc: 0.5570\n",
      "Val Loss: 0.0321 Val Acc: 0.5098 ...\n",
      "Epoch: 7 / 19 ===============================================\n",
      "Epoch 7 completed in 50.82 seconds ! Loss: 0.0271 Acc: 0.5628\n",
      "Val Loss: 0.0320 Val Acc: 0.5113 ...\n",
      "Epoch: 8 / 19 ===============================================\n",
      "Epoch 8 completed in 50.76 seconds ! Loss: 0.0268 Acc: 0.5670\n",
      "Val Loss: 0.0320 Val Acc: 0.5121 ...\n",
      "Epoch: 9 / 19 ===============================================\n",
      "Epoch 9 completed in 50.81 seconds ! Loss: 0.0264 Acc: 0.5737\n",
      "Val Loss: 0.0320 Val Acc: 0.5122 ...\n",
      "Epoch: 10 / 19 ===============================================\n",
      "Epoch 10 completed in 50.73 seconds ! Loss: 0.0260 Acc: 0.5806\n",
      "Val Loss: 0.0320 Val Acc: 0.5120 ...\n",
      "Epoch: 11 / 19 ===============================================\n",
      "Epoch 11 completed in 50.68 seconds ! Loss: 0.0259 Acc: 0.5815\n",
      "Val Loss: 0.0319 Val Acc: 0.5122 ...\n",
      "Epoch: 12 / 19 ===============================================\n",
      "Epoch 12 completed in 50.83 seconds ! Loss: 0.0259 Acc: 0.5837\n",
      "Val Loss: 0.0319 Val Acc: 0.5126 ...\n",
      "Epoch: 13 / 19 ===============================================\n",
      "Batch 102/1063 - Loss: 0.0313 Acc: 0.5156 - Time : 0.05s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-77f1686c00a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_vgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 shuffle = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-776af53eb762>\u001b[0m in \u001b[0;36mtrain_model_2\u001b[0;34m(model, criterion, train_data, train_labels, test_data, test_labels, optimizer, epochs, train, validate, shuffle)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mbatch_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model_2(model=model_vgg.classifier,criterion=criterion,\n",
    "              train_data = conv_feat_train, train_labels = labels_train,\n",
    "                 test_data = conv_feat_val, test_labels = labels_val,\n",
    "                  optimizer = optimizer_vgg,\n",
    "                 epochs = 20,train = True, validate = True,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer_vgg = torch.optim.SGD(model_vgg.classifier.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 4 ===============================================\n",
      "Epoch 0 completed in 50.82 seconds ! Loss: 0.0273 Acc: 0.5589\n",
      "Val Loss: 0.0322 Val Acc: 0.5075 ...\n",
      "Epoch: 1 / 4 ===============================================\n",
      "Batch 135/1063 - Loss: 0.0205 Acc: 0.6875 - Time : 0.05s\r"
     ]
    }
   ],
   "source": [
    "train_model_2(model=model_vgg.classifier,criterion=criterion,\n",
    "              train_data = conv_feat_train, train_labels = labels_train,\n",
    "                 test_data = conv_feat_val, test_labels = labels_val,\n",
    "                  optimizer = optimizer_vgg,\n",
    "                 epochs = 5,train = True, validate = True,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0333 Val Acc: 0.5242 ...\n"
     ]
    }
   ],
   "source": [
    "train_model_2(model=model_vgg.classifier,criterion=criterion,\n",
    "              train_data = conv_feat_train, train_labels = labels_train,\n",
    "                 test_data = conv_feat_val, test_labels = labels_val,\n",
    "                  optimizer = optimizer_vgg,\n",
    "                 epochs = 10,train = False, validate = True,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss & accuracy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_hist)\n",
    "plt.title(\"Loss history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_hist)\n",
    "plt.title(\"Training accuracy history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355 / 355 batches processed\r"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "num_classes = len(dsets['train'].classes)\n",
    "confu_matrix = np.zeros((num_classes,num_classes))\n",
    "\n",
    "model_vgg.eval()\n",
    "\n",
    "for i, data in enumerate(dset_loaders['test'], 0):\n",
    "    start_time = time.time()\n",
    "                \n",
    "    # get the inputs\n",
    "    inputs, classes = data\n",
    "\n",
    "    if use_gpu:\n",
    "        inputs , classes = inputs.cuda(), classes.cuda()\n",
    "\n",
    "    outputs = model_vgg(inputs)\n",
    "\n",
    "    _,preds = torch.max(outputs.data,1)\n",
    "    \n",
    "    confu_matrix += sklearn.metrics.confusion_matrix(classes.cpu(), preds.cpu(),labels=range(num_classes))\n",
    "    \n",
    "    print(i+1,\"/\",len(dset_loaders['test']),\"batches processed\",end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviled_eggs: 0.8558951965065502\n",
      "guacamole: 0.8114035087719298\n",
      "mussels: 0.8076923076923077\n",
      "spaghetti_carbonara: 0.7844036697247706\n",
      "spaghetti_bolognese: 0.7605633802816901\n",
      "pulled_pork_sandwich: 0.7410714285714286\n",
      "strawberry_shortcake: 0.7174887892376681\n",
      "chicken_wings: 0.7168949771689498\n",
      "caesar_salad: 0.7136563876651982\n",
      "macarons: 0.696969696969697\n",
      "eggs_benedict: 0.6854460093896714\n",
      "chocolate_cake: 0.6853448275862069\n",
      "waffles: 0.6680851063829787\n",
      "onion_rings: 0.663594470046083\n",
      "prime_rib: 0.6545454545454545\n",
      "lasagna: 0.6523605150214592\n",
      "miso_soup: 0.6519823788546255\n",
      "caprese_salad: 0.6454545454545455\n",
      "creme_brulee: 0.64\n",
      "panna_cotta: 0.64\n",
      "grilled_cheese_sandwich: 0.6375545851528385\n",
      "clam_chowder: 0.6339285714285714\n",
      "baklava: 0.6327433628318584\n",
      "paella: 0.631578947368421\n",
      "pancakes: 0.6239316239316239\n",
      "pizza: 0.6127659574468085\n",
      "red_velvet_cake: 0.6123348017621145\n",
      "fried_rice: 0.611353711790393\n",
      "bruschetta: 0.6103896103896104\n",
      "seaweed_salad: 0.6073059360730594\n",
      "beet_salad: 0.6071428571428571\n",
      "cup_cakes: 0.6052631578947368\n",
      "churros: 0.6\n",
      "pad_thai: 0.6\n",
      "french_onion_soup: 0.5874439461883408\n",
      "tiramisu: 0.5862068965517241\n",
      "beef_carpaccio: 0.581081081081081\n",
      "french_fries: 0.5754716981132075\n",
      "samosa: 0.5720930232558139\n",
      "fried_calamari: 0.5707547169811321\n",
      "poutine: 0.5666666666666667\n",
      "baby_back_ribs: 0.5656108597285068\n",
      "greek_salad: 0.5605381165919282\n",
      "donuts: 0.5603448275862069\n",
      "pho: 0.5596330275229358\n",
      "edamame: 0.5584415584415584\n",
      "croque_madame: 0.5580357142857143\n",
      "macaroni_and_cheese: 0.5462184873949579\n",
      "club_sandwich: 0.5454545454545454\n",
      "beignets: 0.5357142857142857\n",
      "spring_rolls: 0.5347826086956522\n",
      "nachos: 0.5267857142857143\n",
      "takoyaki: 0.5246636771300448\n",
      "lobster_bisque: 0.5228426395939086\n",
      "sushi: 0.5063291139240507\n",
      "cannoli: 0.4956521739130435\n",
      "frozen_yogurt: 0.49356223175965663\n",
      "hummus: 0.49145299145299143\n",
      "tacos: 0.4892703862660944\n",
      "chicken_quesadilla: 0.4863636363636364\n",
      "garlic_bread: 0.48214285714285715\n",
      "cheesecake: 0.48068669527896996\n",
      "french_toast: 0.4789915966386555\n",
      "omelette: 0.47186147186147187\n",
      "shrimp_and_grits: 0.4692982456140351\n",
      "bread_pudding: 0.4646017699115044\n",
      "cheese_plate: 0.45982142857142855\n",
      "carrot_cake: 0.4588744588744589\n",
      "crab_cakes: 0.45701357466063347\n",
      "breakfast_burrito: 0.4532710280373832\n",
      "falafel: 0.4517543859649123\n",
      "chicken_curry: 0.45045045045045046\n",
      "ceviche: 0.44493392070484583\n",
      "bibimbap: 0.43555555555555553\n",
      "oysters: 0.4349775784753363\n",
      "filet_mignon: 0.4339622641509434\n",
      "apple_pie: 0.43162393162393164\n",
      "gnocchi: 0.43162393162393164\n",
      "ramen: 0.43103448275862066\n",
      "grilled_salmon: 0.4266666666666667\n",
      "chocolate_mousse: 0.42410714285714285\n",
      "hot_and_sour_soup: 0.4187192118226601\n",
      "scallops: 0.41228070175438597\n",
      "risotto: 0.3855932203389831\n",
      "fish_and_chips: 0.3798076923076923\n",
      "gyoza: 0.3706896551724138\n",
      "huevos_rancheros: 0.3632286995515695\n",
      "hamburger: 0.34763948497854075\n",
      "dumplings: 0.34649122807017546\n",
      "pork_chop: 0.3392070484581498\n",
      "escargots: 0.33663366336633666\n",
      "tuna_tartare: 0.33175355450236965\n",
      "steak: 0.3231441048034934\n",
      "lobster_roll_sandwich: 0.3209302325581395\n",
      "ice_cream: 0.31896551724137934\n",
      "peking_duck: 0.3080568720379147\n",
      "beef_tartare: 0.29777777777777775\n",
      "foie_gras: 0.2777777777777778\n",
      "sashimi: 0.26991150442477874\n",
      "ravioli: 0.25\n",
      "hot_dog: 0.13247863247863248\n"
     ]
    }
   ],
   "source": [
    "acc_dict = {}\n",
    "acc = []\n",
    "\n",
    "for i in range(len(dsets['train'].classes)) :\n",
    "    acc_dict[dsets['train'].classes[i]] = float(confu_matrix[i][i])/np.sum(confu_matrix[i])\n",
    "    acc.append(float(confu_matrix[i][i])/np.sum(confu_matrix[i]))\n",
    "    \n",
    "for key, value in sorted(acc_dict.items(), key=lambda kv: kv[1],reverse = True):\n",
    "    print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try weighted number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [float(round(100*x))/100 for x in acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/foodlovers/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "weights = nn.Softmax()(1. / torch.tensor(acc, dtype=torch.float).clone().detach())\n",
    "train_labels = [item[1] for item in dsets['train'].imgs]\n",
    "sample_weights = weights[train_labels]\n",
    "\n",
    "weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, dset_sizes['train'])\n",
    "\n",
    "sampler={'train': weighted_sampler, 'test': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=64,\n",
    "                                                num_workers=6, sampler = sampler[x])\n",
    "                for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.zeros(101)\n",
    "\n",
    "for data in dset_loaders['train']:\n",
    "    inputs_try,labels_try = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 58,  77,  55,  55,  55,  55,  55,  55,  83,  23,  55,  55,  55,  55,\n",
       "         55, 100,  55,  55,  55,  55,  93,  55,  55,  47,  55,  55,  55,  55,\n",
       "         55,  55,  55,  84,  55,  55,  55,  55,  55,  55,  55,  16,  55,  55,\n",
       "         55,  55,  55,   4,   3,  55,  46,  55,  90,  55,  82,  55,  55,  55,\n",
       "         55,  63,  32,  55,  24,  55,  55,  53])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model_vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model_vgg.classifier._modules['6'] = nn.Linear(4096, 101)\n",
    "\n",
    "if use_gpu:\n",
    "    model_vgg = model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 9s, sys: 5min 23s, total: 15min 32s\n",
      "Wall time: 15min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_feat_train,labels_train = preconvfeat(dset_loaders['train'])\n",
    "conv_feat_val,labels_val = preconvfeat(dset_loaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optimizer_vgg = torch.optim.SGD(model_vgg.classifier.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 19 ===============================================\n",
      "Epoch 0 completed in 50.67 seconds ! Loss: 0.0245 Acc: 0.7121\n",
      "Val Loss: 0.0558 Val Acc: 0.1122 ...\n",
      "Epoch: 1 / 19 ===============================================\n",
      "Epoch 1 completed in 50.48 seconds ! Loss: 0.0190 Acc: 0.7352\n",
      "Val Loss: 0.0477 Val Acc: 0.2407 ...\n",
      "Epoch: 2 / 19 ===============================================\n",
      "Epoch 2 completed in 50.84 seconds ! Loss: 0.0163 Acc: 0.7599\n",
      "Val Loss: 0.0462 Val Acc: 0.2830 ...\n",
      "Epoch: 3 / 19 ===============================================\n",
      "Epoch 3 completed in 51.87 seconds ! Loss: 0.0143 Acc: 0.7837\n",
      "Val Loss: 0.0455 Val Acc: 0.3050 ...\n",
      "Epoch: 4 / 19 ===============================================\n",
      "Epoch 4 completed in 50.37 seconds ! Loss: 0.0127 Acc: 0.8020\n",
      "Val Loss: 0.0439 Val Acc: 0.3382 ...\n",
      "Epoch: 5 / 19 ===============================================\n",
      "Epoch 5 completed in 49.99 seconds ! Loss: 0.0113 Acc: 0.8206\n",
      "Val Loss: 0.0427 Val Acc: 0.3616 ...\n",
      "Epoch: 6 / 19 ===============================================\n",
      "Epoch 6 completed in 50.31 seconds ! Loss: 0.0100 Acc: 0.8378\n",
      "Val Loss: 0.0414 Val Acc: 0.3803 ...\n",
      "Epoch: 7 / 19 ===============================================\n",
      "Epoch 7 completed in 51.11 seconds ! Loss: 0.0089 Acc: 0.8546\n",
      "Val Loss: 0.0416 Val Acc: 0.3913 ...\n",
      "Epoch: 8 / 19 ===============================================\n",
      "Epoch 8 completed in 51.08 seconds ! Loss: 0.0079 Acc: 0.8694\n",
      "Val Loss: 0.0423 Val Acc: 0.3876 ...\n",
      "Epoch: 9 / 19 ===============================================\n",
      "Epoch 9 completed in 50.06 seconds ! Loss: 0.0069 Acc: 0.8843\n",
      "Val Loss: 0.0425 Val Acc: 0.3969 ...\n",
      "Epoch: 10 / 19 ===============================================\n",
      "Epoch 10 completed in 50.41 seconds ! Loss: 0.0061 Acc: 0.8980\n",
      "Val Loss: 0.0461 Val Acc: 0.3774 ...\n",
      "Epoch: 11 / 19 ===============================================\n",
      "Epoch 11 completed in 50.32 seconds ! Loss: 0.0053 Acc: 0.9106\n",
      "Val Loss: 0.0435 Val Acc: 0.3990 ...\n",
      "Epoch: 12 / 19 ===============================================\n",
      "Epoch 12 completed in 51.08 seconds ! Loss: 0.0046 Acc: 0.9223\n",
      "Val Loss: 0.0442 Val Acc: 0.3947 ...\n",
      "Epoch: 13 / 19 ===============================================\n",
      "Epoch 13 completed in 49.92 seconds ! Loss: 0.0040 Acc: 0.9336\n",
      "Val Loss: 0.0457 Val Acc: 0.3953 ...\n",
      "Epoch: 14 / 19 ===============================================\n",
      "Epoch 14 completed in 50.33 seconds ! Loss: 0.0035 Acc: 0.9428\n",
      "Val Loss: 0.0457 Val Acc: 0.3997 ...\n",
      "Epoch: 15 / 19 ===============================================\n",
      "Epoch 15 completed in 51.08 seconds ! Loss: 0.0030 Acc: 0.9528\n",
      "Val Loss: 0.0461 Val Acc: 0.4013 ...\n",
      "Epoch: 16 / 19 ===============================================\n",
      "Epoch 16 completed in 50.29 seconds ! Loss: 0.0026 Acc: 0.9588\n",
      "Val Loss: 0.0492 Val Acc: 0.3924 ...\n",
      "Epoch: 17 / 19 ===============================================\n",
      "Batch 1016/1063 - Loss: 0.0027 Acc: 0.9688 - Time : 0.05s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-77f1686c00a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_vgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 shuffle = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-ad81660424c1>\u001b[0m in \u001b[0;36mtrain_model_2\u001b[0;34m(model, criterion, train_data, train_labels, test_data, test_labels, optimizer, epochs, train, validate, shuffle)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mbatch_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model_2(model=model_vgg.classifier,criterion=criterion,\n",
    "              train_data = conv_feat_train, train_labels = labels_train,\n",
    "                 test_data = conv_feat_val, test_labels = labels_val,\n",
    "                  optimizer = optimizer_vgg,\n",
    "                 epochs = 20,train = True, validate = True,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guacamole: 0.7543859649122807\n",
      "deviled_eggs: 0.6768558951965066\n",
      "spaghetti_carbonara: 0.6697247706422018\n",
      "mussels: 0.6196581196581197\n",
      "hot_dog: 0.6025641025641025\n",
      "strawberry_shortcake: 0.5874439461883408\n",
      "waffles: 0.5787234042553191\n",
      "caesar_salad: 0.5726872246696035\n",
      "spaghetti_bolognese: 0.5539906103286385\n",
      "fried_rice: 0.5283842794759825\n",
      "macarons: 0.5281385281385281\n",
      "pulled_pork_sandwich: 0.5223214285714286\n",
      "tiramisu: 0.5172413793103449\n",
      "french_fries: 0.5047169811320755\n",
      "onion_rings: 0.5023041474654378\n",
      "beet_salad: 0.48214285714285715\n",
      "chocolate_mousse: 0.48214285714285715\n",
      "chicken_wings: 0.4794520547945205\n",
      "chicken_quesadilla: 0.4727272727272727\n",
      "donuts: 0.4698275862068966\n",
      "pad_thai: 0.46956521739130436\n",
      "creme_brulee: 0.4666666666666667\n",
      "ravioli: 0.4661016949152542\n",
      "eggs_benedict: 0.4647887323943662\n",
      "pho: 0.463302752293578\n",
      "miso_soup: 0.46255506607929514\n",
      "pizza: 0.4553191489361702\n",
      "cup_cakes: 0.4473684210526316\n",
      "pancakes: 0.4444444444444444\n",
      "edamame: 0.43722943722943725\n",
      "samosa: 0.4372093023255814\n",
      "paella: 0.4342105263157895\n",
      "nachos: 0.4330357142857143\n",
      "red_velvet_cake: 0.43171806167400884\n",
      "spring_rolls: 0.43043478260869567\n",
      "club_sandwich: 0.42727272727272725\n",
      "churros: 0.4260869565217391\n",
      "garlic_bread: 0.42410714285714285\n",
      "greek_salad: 0.42152466367713004\n",
      "scallops: 0.42105263157894735\n",
      "carrot_cake: 0.4155844155844156\n",
      "caprese_salad: 0.41363636363636364\n",
      "hot_and_sour_soup: 0.4088669950738916\n",
      "sashimi: 0.4026548672566372\n",
      "steak: 0.4017467248908297\n",
      "fried_calamari: 0.4009433962264151\n",
      "ice_cream: 0.39655172413793105\n",
      "lasagna: 0.3948497854077253\n",
      "bruschetta: 0.3939393939393939\n",
      "bibimbap: 0.39111111111111113\n",
      "prime_rib: 0.39090909090909093\n",
      "poutine: 0.38571428571428573\n",
      "risotto: 0.3855932203389831\n",
      "tuna_tartare: 0.38388625592417064\n",
      "peking_duck: 0.3744075829383886\n",
      "tacos: 0.37339055793991416\n",
      "falafel: 0.37280701754385964\n",
      "chocolate_cake: 0.3706896551724138\n",
      "seaweed_salad: 0.3698630136986301\n",
      "grilled_cheese_sandwich: 0.3624454148471616\n",
      "beignets: 0.36160714285714285\n",
      "escargots: 0.3564356435643564\n",
      "breakfast_burrito: 0.35046728971962615\n",
      "clam_chowder: 0.3482142857142857\n",
      "ceviche: 0.3436123348017621\n",
      "pork_chop: 0.3436123348017621\n",
      "omelette: 0.341991341991342\n",
      "baklava: 0.3407079646017699\n",
      "lobster_bisque: 0.3401015228426396\n",
      "macaroni_and_cheese: 0.33613445378151263\n",
      "cannoli: 0.3347826086956522\n",
      "hamburger: 0.33476394849785407\n",
      "french_onion_soup: 0.33183856502242154\n",
      "baby_back_ribs: 0.33031674208144796\n",
      "apple_pie: 0.32905982905982906\n",
      "bread_pudding: 0.3274336283185841\n",
      "cheesecake: 0.3261802575107296\n",
      "shrimp_and_grits: 0.32456140350877194\n",
      "beef_tartare: 0.3244444444444444\n",
      "sushi: 0.3206751054852321\n",
      "panna_cotta: 0.32\n",
      "crab_cakes: 0.3167420814479638\n",
      "foie_gras: 0.3148148148148148\n",
      "oysters: 0.31390134529147984\n",
      "beef_carpaccio: 0.3108108108108108\n",
      "chicken_curry: 0.3063063063063063\n",
      "french_toast: 0.3025210084033613\n",
      "grilled_salmon: 0.29777777777777775\n",
      "filet_mignon: 0.29245283018867924\n",
      "gyoza: 0.28879310344827586\n",
      "ramen: 0.2801724137931034\n",
      "croque_madame: 0.2767857142857143\n",
      "huevos_rancheros: 0.25112107623318386\n",
      "hummus: 0.24358974358974358\n",
      "dumplings: 0.2412280701754386\n",
      "takoyaki: 0.23766816143497757\n",
      "fish_and_chips: 0.23557692307692307\n",
      "cheese_plate: 0.23214285714285715\n",
      "gnocchi: 0.23076923076923078\n",
      "frozen_yogurt: 0.22317596566523606\n",
      "lobster_roll_sandwich: 0.16744186046511628\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(dsets['train'].classes)\n",
    "confu_matrix = np.zeros((num_classes,num_classes))\n",
    "\n",
    "model_vgg.eval()\n",
    "\n",
    "for i, data in enumerate(dset_loaders['test'], 0):\n",
    "    start_time = time.time()\n",
    "                \n",
    "    # get the inputs\n",
    "    inputs, classes = data\n",
    "\n",
    "    if use_gpu:\n",
    "        inputs , classes = inputs.cuda(), classes.cuda()\n",
    "\n",
    "    outputs = model_vgg(inputs)\n",
    "\n",
    "    _,preds = torch.max(outputs.data,1)\n",
    "    \n",
    "    confu_matrix += sklearn.metrics.confusion_matrix(classes.cpu(), preds.cpu(),labels=range(num_classes))\n",
    "    \n",
    "    print(i+1,\"/\",len(dset_loaders['test']),\"batches processed\",end=\"\\r\")\n",
    "    \n",
    "acc_dict2 = {}\n",
    "\n",
    "for i in range(len(dsets['train'].classes)) :\n",
    "    acc_dict2[dsets['train'].classes[i]] = float(confu_matrix[i][i])/np.sum(confu_matrix[i])\n",
    "    \n",
    "for key, value in sorted(acc_dict2.items(), key=lambda kv: kv[1],reverse = True):\n",
    "    print(\"%s: %s\" % (key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
