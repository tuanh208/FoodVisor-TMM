{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = '/home/foodlovers/FoodVisor/data/texts_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_texts = datasets.load_files(text_dir, \n",
    "            description=None, categories=None, load_content=True, shuffle=False, \n",
    "                                        encoding='utf-8', decode_error='strict', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/foodlovers/FoodVisor/data/texts_txt/apple_pie/apple_pie_0.txt',\n",
       "       '/home/foodlovers/FoodVisor/data/texts_txt/apple_pie/apple_pie_1.txt',\n",
       "       '/home/foodlovers/FoodVisor/data/texts_txt/apple_pie/apple_pie_10.txt',\n",
       "       '/home/foodlovers/FoodVisor/data/texts_txt/apple_pie/apple_pie_100.txt',\n",
       "       '/home/foodlovers/FoodVisor/data/texts_txt/apple_pie/apple_pie_101.txt'],\n",
       "      dtype='<U97')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.target_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = all_texts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93409"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/foodlovers/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 45s, sys: 6.24 s, total: 56min 51s\n",
      "Wall time: 56min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proc_docs = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    word_tokens = word_tokenize(doc) \n",
    "    processed = [w for w in word_tokens if not w in stop_words]\n",
    "    processed = [porter.stem(w) for w in word_tokens]\n",
    "    proc_docs.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 20s, sys: 850 ms, total: 4min 21s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for doc in proc_docs:\n",
    "    for i in range(len(doc)) :\n",
    "        doc[i] = doc[i].lstrip().rstrip()\n",
    "        doc[i] = re.sub(r\"[\\^\\$\\-()\\\"#/@;:<>{}`+=~|\\]\\[._\\\\!?,%&*0-9]\", \"\", doc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(corps):\n",
    "    dic = {}\n",
    "    for doc in corps:\n",
    "        for w in doc :\n",
    "            dic[w] = dic.get(w, 0) + 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = words_count(proc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_doc = dict()\n",
    "for doc in proc_docs:\n",
    "    doc = set(doc)\n",
    "    for w in doc :\n",
    "        vocab_doc[w] = vocab_doc.get(w, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048895"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['recip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89548"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_doc['recip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821569"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821569"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(count, get = 20, reverse=True) :\n",
    "    c = 0\n",
    "    for w in sorted(count, key=count.get, reverse=reverse):\n",
    "        c += 1\n",
    "        if c > get :\n",
    "            break\n",
    "        print(w,\": \", count[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_words(count, below, top) :\n",
    "    \"\"\"\n",
    "    Remove words that occur less than (and more than) a number of time  \\n\n",
    "    and return the removed words.\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    removed_words = []\n",
    "    # bottom up\n",
    "    for w in sorted(count, key=count.get, reverse=False):\n",
    "        if count[w] < below :\n",
    "            c += 1\n",
    "            removed_words.append(w)\n",
    "            # print(w,\": \", count[w])\n",
    "        else:\n",
    "            break\n",
    "    # top down\n",
    "    for w in sorted(count, key=count.get, reverse=True):\n",
    "        if count[w] > top :\n",
    "            c += 1\n",
    "            removed_words.append(w)\n",
    "            # print(w,\": \", count[w])\n",
    "        else:\n",
    "            break\n",
    "    print(c, \" words to be removed.\")\n",
    "    return removed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :  92604\n",
      "and :  90534\n",
      "to :  90287\n",
      "the :  90249\n",
      "a :  89866\n",
      "recip :  89548\n",
      "of :  89506\n",
      "in :  88587\n",
      "with :  88375\n",
      "for :  88148\n",
      "on :  85736\n",
      "you :  85132\n",
      "thi :  82973\n",
      "about :  81848\n",
      "your :  79999\n",
      "or :  79697\n",
      "is :  78768\n",
      "all :  78765\n",
      "from :  77758\n",
      "by :  77089\n",
      "'s :  76371\n",
      "it :  76082\n",
      "food :  75827\n",
      "cook :  75138\n",
      "make :  74000\n",
      "use :  73816\n",
      "home :  73352\n",
      "more :  73145\n",
      "are :  71965\n",
      "at :  68976\n",
      "ingredi :  68508\n",
      "be :  67693\n",
      "that :  66451\n",
      "like :  65690\n",
      "serv :  65316\n",
      "can :  65231\n",
      "add :  64172\n",
      "not :  64002\n",
      "contact :  63681\n",
      "time :  63610\n",
      "I :  63080\n",
      "have :  63007\n",
      "new :  62974\n",
      "top :  61516\n",
      "up :  61461\n",
      "email :  61382\n",
      "get :  61087\n",
      "one :  60937\n",
      "will :  60558\n",
      "into :  59733\n",
      "comment :  59208\n",
      "minut :  59148\n",
      "as :  58851\n",
      "out :  58561\n",
      "an :  58551\n",
      "post :  58534\n",
      "how :  57708\n",
      "our :  57190\n",
      "dish :  56933\n",
      "right :  56187\n",
      "blog :  56111\n",
      "easi :  55857\n",
      "search :  55814\n",
      "just :  55647\n",
      "but :  55458\n",
      "A :  55339\n",
      "share :  55115\n",
      "In :  54658\n",
      "over :  54429\n",
      "until :  54104\n",
      "what :  53932\n",
      "love :  53568\n",
      "cup :  53441\n",
      "so :  53367\n",
      "It :  53076\n",
      "C :  52975\n",
      "chicken :  52746\n",
      "sauc :  52734\n",
      "n't :  52700\n",
      "dessert :  52631\n",
      "'' :  51946\n",
      "here :  51186\n",
      "tri :  50840\n",
      "bake :  50772\n",
      "tast :  50737\n",
      "best :  50070\n",
      "salt :  49973\n",
      "facebook :  49950\n",
      "follow :  49931\n",
      "day :  49874\n",
      "privaci :  49778\n",
      "when :  49502\n",
      "good :  49486\n",
      "fresh :  49443\n",
      "my :  49166\n",
      "these :  49011\n",
      "delici :  48873\n",
      "do :  48405\n",
      "side :  48323\n",
      "some :  48017\n",
      "salad :  47954\n",
      "may :  47804\n",
      "now :  47677\n",
      "eat :  47155\n",
      "chees :  46941\n",
      "see :  46723\n",
      "made :  46661\n",
      "other :  46401\n",
      "site :  45971\n",
      "place :  45757\n",
      "My :  45548\n",
      "sweet :  45476\n",
      "well :  45296\n",
      "wa :  45295\n",
      "great :  45188\n",
      "free :  44857\n",
      "egg :  44785\n",
      "favorit :  44567\n",
      "oil :  44322\n",
      "bowl :  44273\n",
      "also :  44198\n",
      "mix :  44185\n",
      "find :  44018\n",
      "heat :  43911\n",
      "if :  43907\n",
      "they :  43821\n",
      "way :  43701\n",
      "them :  43578\n",
      "twitter :  43465\n",
      "then :  43358\n",
      "juli :  43337\n",
      "pepper :  43285\n",
      "reserv :  43276\n",
      "dinner :  43148\n",
      "leav :  43116\n",
      "ani :  42782\n",
      "most :  42711\n",
      "look :  42496\n",
      "hot :  42455\n",
      "kitchen :  42392\n",
      "cake :  42380\n",
      "tip :  42022\n",
      "season :  41927\n",
      "polici :  41479\n",
      "take :  41423\n",
      "me :  41393\n",
      "need :  41131\n",
      "soup :  40919\n",
      "ha :  40712\n",
      "we :  40671\n",
      "healthi :  40605\n",
      "To :  40577\n",
      "bread :  40562\n",
      "cream :  40459\n",
      "We :  40254\n",
      "water :  40254\n",
      "term :  40164\n",
      "If :  39718\n",
      "video :  39574\n",
      "subscrib :  39513\n",
      "chop :  39415\n",
      "larg :  39291\n",
      "popular :  39196\n",
      "breakfast :  39119\n",
      "butter :  39100\n",
      "name :  38938\n",
      "littl :  38829\n",
      "want :  38757\n",
      "red :  38702\n",
      "onli :  38648\n",
      "meal :  38455\n",
      "veget :  38380\n",
      "Us :  38258\n",
      "grill :  37860\n",
      "go :  37827\n",
      "first :  37798\n",
      "work :  37728\n",
      "white :  37615\n",
      "copyright :  37587\n",
      "quick :  37575\n",
      "sugar :  37479\n",
      "slice :  37430\n",
      "us :  37332\n",
      "fri :  37303\n",
      "small :  37022\n",
      "chocol :  36965\n",
      "next :  36938\n",
      "perfect :  36807\n",
      "cut :  36772\n",
      "read :  36754\n",
      "onion :  36748\n",
      "veri :  36589\n",
      "would :  36457\n",
      "pleas :  36414\n",
      "help :  36385\n",
      "advertis :  36290\n",
      "idea :  36277\n",
      "view :  36174\n",
      "photo :  36171\n",
      "there :  36113\n",
      "brown :  36017\n",
      "famili :  35988\n",
      "main :  35976\n",
      "prepar :  35947\n",
      "shop :  35932\n",
      "each :  35857\n",
      "green :  35575\n",
      "know :  35518\n",
      "garlic :  35499\n",
      "enjoy :  35315\n",
      "review :  35273\n",
      "turn :  35151\n",
      "let :  35144\n",
      "summer :  35056\n",
      "list :  35003\n",
      "simpl :  34974\n",
      "print :  34942\n",
      "come :  34921\n",
      "flavor :  34866\n",
      "appet :  34841\n",
      "categori :  34834\n",
      "pan :  34706\n",
      "beef :  34619\n",
      "too :  34522\n",
      "befor :  34413\n",
      "year :  34393\n",
      "' :  34391\n",
      "cooki :  34214\n",
      "pasta :  34189\n",
      "tomato :  34182\n",
      "pinterest :  34158\n",
      "ad :  33978\n",
      "stir :  33946\n",
      "set :  33927\n",
      "than :  33644\n",
      "much :  33424\n",
      "no :  33285\n",
      "back :  33017\n",
      "By :  32951\n",
      "remov :  32945\n",
      "combin :  32913\n",
      "tablespoon :  32784\n",
      "live :  32732\n",
      "vegetarian :  32611\n",
      "relat :  32607\n",
      "potato :  32548\n",
      "give :  32459\n",
      "join :  32366\n",
      "menu :  32333\n",
      "two :  32324\n",
      "websit :  32319\n",
      "realli :  32297\n",
      "sign :  32134\n",
      "thank :  32075\n",
      "thing :  32002\n",
      "'ve :  31889\n",
      "mixtur :  31411\n",
      "which :  31392\n",
      "again :  31346\n",
      "recent :  31254\n",
      "did :  31232\n",
      "been :  31201\n",
      "cancel :  31192\n",
      "few :  31159\n",
      "wine :  31113\n",
      "drink :  30910\n",
      "rice :  30856\n",
      "even :  30834\n",
      "italian :  30759\n",
      "parti :  30704\n",
      "lemon :  30676\n",
      "keep :  30633\n",
      "medium :  30613\n",
      "togeth :  30456\n",
      "am :  30439\n",
      "meat :  30407\n",
      "start :  30384\n",
      "off :  30367\n",
      "dri :  30178\n",
      "holiday :  30153\n",
      "s :  30071\n",
      "hour :  30034\n",
      "put :  30028\n",
      "cover :  30025\n",
      "kid :  29973\n",
      "snack :  29878\n",
      "while :  29836\n",
      "think :  29718\n",
      "sure :  29713\n",
      "had :  29709\n",
      "save :  29662\n",
      "whole :  29637\n",
      "after :  29522\n",
      "click :  29504\n",
      "requir :  29493\n",
      "page :  29437\n",
      "'ll :  29334\n",
      "june :  29263\n",
      "everi :  29262\n",
      "own :  29089\n",
      "cours :  28987\n",
      "oven :  28964\n",
      "featur :  28949\n",
      "oliv :  28924\n",
      "friend :  28898\n",
      "teaspoon :  28819\n",
      "fill :  28798\n",
      "’ :  28613\n",
      "week :  28597\n",
      "restaur :  28587\n",
      "creat :  28580\n",
      "store :  28561\n",
      "pork :  28396\n",
      "fat :  28344\n",
      "say :  28340\n",
      "chef :  28334\n",
      "special :  28323\n",
      "Do :  28243\n",
      "link :  28241\n",
      "should :  28192\n",
      "sandwich :  28150\n",
      "their :  28127\n",
      "cool :  27914\n",
      "direct :  27862\n",
      "roast :  27824\n",
      "log :  27751\n",
      "'re :  27642\n",
      "repli :  27567\n",
      "never :  27485\n",
      "today :  27410\n",
      "content :  27323\n",
      "better :  27284\n",
      "black :  27236\n",
      "someth :  27118\n",
      "ice :  27096\n",
      "doe :  27058\n",
      "boil :  27054\n",
      "'m :  26949\n",
      "power :  26910\n",
      "tag :  26899\n",
      "pie :  26892\n",
      "fish :  26874\n",
      "bean :  26814\n",
      "travel :  26789\n",
      "ground :  26784\n",
      "french :  26756\n",
      "who :  26751\n",
      "fine :  26737\n",
      "half :  26715\n",
      "style :  26680\n",
      "product :  26514\n",
      "through :  26503\n",
      "life :  26493\n",
      "peopl :  26317\n",
      "juic :  26270\n",
      "last :  26228\n",
      "archiv :  26199\n",
      "without :  26194\n",
      "box :  26177\n",
      "april :  26159\n",
      "march :  26065\n",
      "low :  25973\n",
      "lunch :  25849\n",
      "bit :  25811\n",
      "could :  25725\n",
      "bring :  25715\n",
      "No :  25700\n",
      "februari :  25634\n",
      "becaus :  25593\n",
      "full :  25588\n",
      "januari :  25540\n",
      "check :  25507\n",
      "anoth :  25492\n",
      "total :  25453\n",
      "newslett :  25447\n",
      "homemad :  25339\n",
      "book :  25329\n",
      "rss :  25280\n",
      "light :  25206\n",
      "octob :  25156\n",
      "down :  25114\n",
      "press :  25113\n",
      "fruit :  25068\n",
      "ever :  25065\n",
      "flour :  25058\n",
      "rate :  25049\n",
      "roll :  25009\n",
      "around :  24976\n",
      "address :  24902\n",
      "seafood :  24899\n",
      "where :  24880\n",
      "decemb :  24829\n",
      "world :  24826\n",
      "beauti :  24719\n",
      "month :  24663\n",
      "alway :  24597\n",
      "august :  24539\n",
      "select :  24533\n",
      "» :  24520\n",
      "mani :  24510\n",
      "person :  24495\n",
      "onc :  24482\n",
      "milk :  24481\n",
      "novemb :  24431\n",
      "piec :  24395\n",
      "were :  24326\n",
      "under :  24235\n",
      "warm :  24185\n",
      "show :  24114\n",
      "fun :  24080\n",
      "dip :  24043\n",
      "septemb :  24013\n",
      "So :  23883\n",
      "part :  23875\n",
      "whi :  23842\n",
      "sprinkl :  23693\n",
      "learn :  23554\n",
      "note :  23543\n",
      "classic :  23346\n",
      "health :  23313\n",
      "pizza :  23248\n",
      "pour :  23117\n",
      "chang :  23070\n",
      "On :  22932\n",
      "cuisin :  22886\n",
      "pot :  22876\n",
      "big :  22864\n",
      "still :  22826\n",
      "got :  22702\n",
      "high :  22666\n",
      "american :  22478\n",
      "enough :  22458\n",
      "enter :  22445\n",
      "lot :  22331\n",
      "hand :  22304\n",
      "th :  22244\n",
      "powder :  22236\n",
      "complet :  22201\n",
      "long :  22106\n",
      "close :  21975\n",
      "inspir :  21867\n",
      "entertain :  21828\n",
      "appl :  21814\n",
      "spice :  21808\n",
      "publish :  21790\n",
      "commun :  21763\n",
      "spici :  21737\n",
      "second :  21694\n",
      "g :  21664\n",
      "celebr :  21644\n",
      "plate :  21635\n",
      "found :  21543\n",
      "cookbook :  21465\n",
      "inform :  21462\n",
      "thought :  21443\n",
      "remain :  21420\n",
      "melt :  21394\n",
      "design :  21331\n",
      "bacon :  21286\n",
      "tweet :  21261\n",
      "amaz :  21206\n",
      "nutrit :  21118\n",
      "nice :  21113\n",
      "diet :  21063\n",
      "differ :  21018\n",
      "those :  21016\n",
      "creami :  21013\n",
      "readi :  20971\n",
      "latest :  20918\n",
      "wonder :  20878\n",
      "strawberri :  20823\n",
      "plan :  20815\n",
      "mexican :  20796\n",
      "http :  20764\n",
      "option :  20763\n",
      "real :  20645\n",
      "faq :  20622\n",
      "tasti :  20584\n",
      "mushroom :  20556\n",
      "dress :  20555\n",
      "feel :  20550\n",
      "everyth :  20545\n",
      "extra :  20519\n",
      "visit :  20447\n",
      "connect :  20439\n",
      "care :  20324\n",
      "night :  20281\n",
      "prep :  20277\n",
      "guid :  20160\n",
      "golden :  20104\n",
      "updat :  20092\n",
      "t :  20078\n",
      "happi :  20040\n",
      "magazin :  20026\n",
      "might :  19977\n",
      "end :  19963\n",
      "per :  19952\n",
      "christma :  19798\n",
      "size :  19775\n",
      "recommend :  19736\n"
     ]
    }
   ],
   "source": [
    "top_words(vocab_doc,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piemavencom :  1\n",
      "acreliu :  1\n",
      "topandbottom :  1\n",
      "boecxken :  1\n",
      "ochef :  1\n",
      "ginster :  1\n",
      "'een :  1\n",
      "kookhistorienl :  1\n",
      "wwwusappleorgconsumersallaboutappleshistoryandfolkloreapplesinamerica :  1\n",
      "'rehabilitate' :  1\n",
      "farlex :  1\n",
      "boardsstraightdopecom :  1\n",
      "sorghvuldig :  1\n",
      "sucrosecom :  1\n",
      "cokeryen :  1\n",
      "notabel :  1\n",
      "piemaven :  1\n",
      "americanet :  1\n",
      "huyshoudst :  1\n",
      "verstandig :  1\n",
      "pietowncom :  1\n",
      "titleapplepi :  1\n",
      "cofyn :  1\n",
      "piefil :  1\n",
      "wwwtasteofhomecomrecipesapple :  1\n",
      "sgronholz :  1\n",
      "pielover :  1\n",
      "peasshell :  1\n",
      "summerlili :  1\n",
      "mumsy :  1\n",
      "catmom :  1\n",
      "extractit :  1\n",
      "tinngo :  1\n",
      "berniciu :  1\n",
      "watcm :  1\n",
      "shcoke :  1\n",
      "streussel :  1\n",
      "grann :  1\n",
      "raisinfre :  1\n",
      "bananabobana :  1\n",
      "noraisinsonmyparad :  1\n",
      "kidstir :  1\n",
      "appetizerthem :  1\n",
      "jeannieallen :  1\n",
      "halfhil :  1\n",
      "mamest :  1\n",
      "nevou :  1\n",
      "buffaloshrimp :  1\n",
      "dragonflies :  1\n",
      "'couples :  1\n"
     ]
    }
   ],
   "source": [
    "top_words(vocab_doc,50,reverse =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757005  words to be removed.\n"
     ]
    }
   ],
   "source": [
    "removed_words = slice_words(vocab_doc, 15,53000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :  54754376\n",
      "the :  4523309\n",
      "and :  3516273\n",
      "to :  2747460\n",
      "a :  2515936\n",
      "recip :  2048895\n",
      "I :  2002130\n",
      "of :  1840784\n",
      "in :  1430492\n",
      "with :  1375447\n",
      "for :  1372872\n",
      "you :  1246047\n",
      "it :  1204130\n",
      "thi :  971061\n",
      "is :  871674\n",
      "on :  865717\n",
      "at :  841081\n",
      "'' :  803414\n",
      "your :  752276\n",
      "'s :  734711\n",
      "or :  715532\n",
      "that :  638427\n",
      "from :  561102\n",
      "make :  543725\n",
      "cook :  526267\n",
      "food :  525307\n",
      "have :  494445\n",
      "use :  471031\n",
      "repli :  470176\n",
      "my :  466193\n",
      "are :  458809\n",
      "more :  451874\n",
      "all :  436862\n",
      "by :  414062\n",
      "like :  411861\n",
      "but :  408586\n",
      "about :  408205\n",
      "wa :  405334\n",
      "be :  403440\n",
      "post :  389923\n",
      "as :  360298\n",
      "so :  356028\n",
      "add :  354571\n",
      "comment :  354438\n",
      "can :  353177\n",
      "not :  351826\n",
      "n't :  347864\n",
      "cup :  321719\n",
      "chicken :  307887\n",
      "love :  307836\n"
     ]
    }
   ],
   "source": [
    "top_words(vocab,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743616  words to be removed.\n"
     ]
    }
   ],
   "source": [
    "removed_words_2 = slice_words(vocab, 15,340000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743611"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(removed_words).intersection(set(removed_words_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = stop_words.union(set(removed_words)).union(set(removed_words_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 93409\n",
      "2000 / 93409\n",
      "3000 / 93409\n",
      "4000 / 93409\n",
      "5000 / 93409\n",
      "6000 / 93409\n",
      "7000 / 93409\n",
      "8000 / 93409\n",
      "9000 / 93409\n",
      "10000 / 93409\n",
      "11000 / 93409\n",
      "12000 / 93409\n",
      "13000 / 93409\n",
      "14000 / 93409\n",
      "15000 / 93409\n",
      "16000 / 93409\n",
      "17000 / 93409\n",
      "18000 / 93409\n",
      "19000 / 93409\n",
      "20000 / 93409\n",
      "21000 / 93409\n",
      "22000 / 93409\n",
      "23000 / 93409\n",
      "24000 / 93409\n",
      "25000 / 93409\n",
      "26000 / 93409\n",
      "27000 / 93409\n",
      "28000 / 93409\n",
      "29000 / 93409\n",
      "30000 / 93409\n",
      "31000 / 93409\n",
      "32000 / 93409\n",
      "33000 / 93409\n",
      "34000 / 93409\n",
      "35000 / 93409\n",
      "36000 / 93409\n",
      "37000 / 93409\n",
      "38000 / 93409\n",
      "39000 / 93409\n",
      "40000 / 93409\n",
      "41000 / 93409\n",
      "42000 / 93409\n",
      "43000 / 93409\n",
      "44000 / 93409\n",
      "45000 / 93409\n",
      "46000 / 93409\n",
      "47000 / 93409\n",
      "48000 / 93409\n",
      "49000 / 93409\n",
      "50000 / 93409\n",
      "51000 / 93409\n",
      "52000 / 93409\n",
      "53000 / 93409\n",
      "54000 / 93409\n",
      "55000 / 93409\n",
      "56000 / 93409\n",
      "57000 / 93409\n",
      "58000 / 93409\n",
      "59000 / 93409\n",
      "60000 / 93409\n",
      "61000 / 93409\n",
      "62000 / 93409\n",
      "63000 / 93409\n",
      "64000 / 93409\n",
      "65000 / 93409\n",
      "66000 / 93409\n",
      "67000 / 93409\n",
      "68000 / 93409\n",
      "69000 / 93409\n",
      "70000 / 93409\n",
      "71000 / 93409\n",
      "72000 / 93409\n",
      "73000 / 93409\n",
      "74000 / 93409\n",
      "75000 / 93409\n",
      "76000 / 93409\n",
      "77000 / 93409\n",
      "78000 / 93409\n",
      "79000 / 93409\n",
      "80000 / 93409\n",
      "81000 / 93409\n",
      "82000 / 93409\n",
      "83000 / 93409\n",
      "84000 / 93409\n",
      "85000 / 93409\n",
      "86000 / 93409\n",
      "87000 / 93409\n",
      "88000 / 93409\n",
      "89000 / 93409\n",
      "90000 / 93409\n",
      "91000 / 93409\n",
      "92000 / 93409\n",
      "93000 / 93409\n",
      "CPU times: user 30.2 s, sys: 309 ms, total: 30.5 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_proc_docs = []\n",
    "\n",
    "cc = 0\n",
    "for doc in proc_docs:\n",
    "    processed = [w for w in doc if (w not in new_stop_words and len(w) > 1)]\n",
    "    new_proc_docs.append(processed)\n",
    "    cc += 1\n",
    "    if cc%1000 == 0 :\n",
    "        print(cc,\"/\",len(proc_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93409"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_proc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = words_count(new_proc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64121"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken :  307887\n",
      "sauc :  306111\n",
      "bake :  305044\n",
      "chees :  294412\n",
      "egg :  290002\n",
      "cake :  289429\n",
      "oil :  283996\n",
      "say :  283091\n",
      "salt :  266564\n",
      "pm :  260971\n",
      "pepper :  257722\n",
      "may :  249001\n",
      "tri :  245260\n",
      "day :  218062\n",
      "thank :  216410\n",
      "good :  214155\n",
      "review :  213865\n",
      "made :  210557\n",
      "cream :  208003\n",
      "look :  204734\n"
     ]
    }
   ],
   "source": [
    "top_words(new_vocab, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridi :  15\n",
      "cinnamonglaz :  15\n",
      "cranberryalmond :  15\n",
      "inextric :  15\n",
      "janetfctc :  15\n",
      "chiot :  15\n",
      "creations… :  15\n",
      "medal™ :  15\n",
      "communiqu :  15\n",
      "mulitpl :  15\n",
      "montara :  15\n",
      "typea :  15\n",
      "luvcook :  15\n",
      "monetarili :  15\n",
      "bannist :  15\n",
      "combination :  15\n",
      "ganache :  15\n",
      "complaining :  15\n",
      "greataunt :  15\n",
      "tfla :  15\n"
     ]
    }
   ],
   "source": [
    "top_words(new_vocab, reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"appl pie wikipedia free encyclopedia jump navig articl see appl pie disambigu appl pie appl pie lattic upper crust type dessert main appl sugar energi per around calori piec appl pie kcal cookbook appl pie appl pie tart tatin french variat appl pie An appl pie fruit pie tart princip fill appl sometim whip cream ice cream alongsid cheddar chees pastri gener doublecrust pie upper crust may circular shape crust pastri lattic woven strip except deepdish appl pie crust onli openfac tart tatin content english pud absenc sugar earli english dutch style swedish style appl pie american cultur see also refer extern link edit appl culinari appl bramley empir northern spi granni smith crisp acid fruit pie fresh reconstitut dri appl affect final textur length requir whether ha effect flavour pie matter opinion dri preserv appl origin substitut onli fresh fruit unavail appl pie often style la mode ice cream altern piec chees sharp cheddar occasion place alongsid slice finish pie english pud edit tarti appli thcenturi print thcenturi english appl pie go back see illustr list good appl good spice fig raisin pear case pastri saffron colour pie fill english speak countri appl pie dessert endur popular eaten hot cold ice cream doubl cream custard absenc sugar earli english edit modern appl pie requir ounc two sugar earliest doe two possibl reason sugarcan import egypt wide avail thcenturi england cost two shill per pound roughli equival US per kg US per pound today' price honey mani cheaper also absent good spice saffron import less expens difficult obtain refin sugar despit expens refin sugar appear much often publish honey suggest consid prohibit expens except appl pear fill probabl import perhap modern sugarfre juic pear intend sweeten pie dutch style edit tradit dutch appl pie come two varieti crumb lattic style pie distinct typic call flavour cinnamon lemon juic ad differ textur tast dutch appl pie may includ raisin ice addit appl sugar common dutch appl pie go back centuri exist paint dutch golden age date featur pie late mediev dutch book van around almost ident modern basi dutch appl pie crust bottom around edg fill piec slice appl usual crisp mildli tart varieti cinnamon sugar gener mix appl fill atop fill strand dough cover pie lattic hold fill place keep visibl cover pie crumb eaten warm cold sometim dash whip cream vanilla ice cream US dutch appl pie refer specif appl pie style crumb streusel swedish style edit swedish style appl pie predominantli varieti appl crumbl rather tradit pastri pie often breadcrumb wholli partial instead flour sometim roll oat usual flavour cinnamon vanilla custard ice cream also veri popular version call appl cake differ pie spong cake bake fresh appl piec appl pie american cultur edit An appl pie number american cultur icon english coloni appl pie wait plant european varieti brought across atlant becom appl tree select qualiti nativ appl meantim colonist pie pasti meat rather fruit main appl onc avail cider howev american appl pie manuscript print th centuri ha sinc becom veri popular dessert appl varieti propag graft clone world plant seed popular quickli lead develop hundr nativ varieti appl pie common thcenturi delawar As note sweden historian dr israel letter appl pie throughout whole year fresh appl longer dri even meal children mock appl pie made cracker possibl invent pioneer move dure th centuri appl mani year afterward ritz cracker promot mock appl pie product along sugar variou spice although appl pie eaten sinc long befor european colonis america american appl pie say unit state mean typic american nineteenth twentieth centuri appl pie becam symbol american prosper nation pride newspap articl publish declar No pie eat peopl perman also commemor phrase mom appl pie supposedli stock answer american soldier world war II whenev journalist ask whi go war jack holden franc kay sang patriot song fieri bear We basebal appl pie We counti fair We 'll keep old glori wave high place bear creat contrast popular view us cultur soviet union advertis exploit patriot connect commerci jingl basebal hot dog appl pie chevrolet claim appl market board york state slogan An appl day keep doctor away american appl pie thu abl success appl popular comest earli th centuri prohibit outlaw product hard cider dubiou discuss commun pie town mexico name honour appl pie see also edit portal apfelstrudel appl strudel austrian made dough appl sugar spice appl cake appl cobbler tart tatin french variant appl pie refer edit appl free dictionari By retriev An appl pie without chees apart therapi retriev appl pie retriev product highlight appl pie sharp cheddar nice coffe hunger mountain coop retriev sugar made histori sugar knowledg intern retriev appl cake aka dutch appl tart tap retriev page De anno retriev van door thoma vander retriev dutch appl pie brown eye baker retriev origin histori cultiv univers georgia archiv origin januari retriev februari center divers genu malu eastern turkey southwestern russia region asia minor appl improv select period thousand year earli farmer alexand great credit find dwarf appl asia minor BC brought back greec may well dwarf appl brought north america colonist first appl orchard contin said near boston http linda appl pie histori appl pie archiv origin june retriev juli By beth put ritz saveurcom retriev cambridg univers press definit american appl pie cambridg advanc learner dictionari thesauru popular appl say us appl associ archiv origin juli retriev juli pie idiom retriev base proverb pembrokeshir date dictionari proverb By john simpson isbn shorten version As american motherhood appl pie unit state pie By adrienn kane isbn american appl pie retriev pie town mexico retriev extern link edit wikimedia common ha media relat appl pie timelin histori note appl pie appl pie kate print children book base much earlier rhyme project american pie sweet appl crisp appl pie bean pie blackberri pie blueberri pie bob andi pie buttermilk pie cherri pie chess pie cream pie derbi pie fri pie key lime pie lemon ice box pie pie mississippi mud pie pecan pie pumpkin pie rhubarb pie shoofli pie strawberri rhubarb pie sweet potato pie savori crawfish pie pot pie meat pie manufactur hostess kellogg compani littl debbi mari callend sara lee tasti bake compani british pie sweet appl pie bakewel tart banoffe pie bedfordshir black bun custard tart manchest tart minc pie rhubarb pie treacl tart savouri bacon egg pie bedfordshir bridi butter pie chicken mushroom pie corn beef pie cornish pasti cottag pie cumberland pie curri pie pie fish pie game pie pie pie meat potato pie melton pork pie pork pie scotch pie shepherd pie squab pie pie steak pie steak kidney pie steak oyster pie pie manufactur clark pie dickinson morri fray bento holland pie Mr kipl peter pool pork farm pukka pie shire squar pie wall wright pie retriev http enwikipediaorgwindexphp oldid categori american pie appl product british dessert british pie dutch cuisin english cuisin german cuisin fruit sweet pie swedish cuisin hidden categori accuraci disput articl disput statement octob common categori local link wikidata dmi date septemb navig menu person tool creat account log namespac articl talk variant view read edit view histori navig main page content featur content current event random articl donat wikipedia wikimedia shop interact help wikipedia commun portal recent chang page tool link relat chang upload file special page perman link page inform data item cite page printexport creat book download pdf printabl version languag العربية català deutsch eesti español esperanto فارسی françai galego 한국어 հայերեն עברית nederland 日本語 norsk bokmål polski portuguê русский simpl english suomi 粵語 中文 edit link page last modifi june text avail creativ common attributionsharealik licens addit term may appli By site agre term privaci polici wikipedia® regist trademark wikimedia foundat inc nonprofit organ privaci polici wikipedia disclaim wikipedia develop mobil view\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(new_proc_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_proc_texts = []\n",
    "for doc in new_proc_docs:\n",
    "    new_proc_texts.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(new_proc_texts) == len(all_texts.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 93409 files saved.\n",
      "1001 / 93409 files saved.\n",
      "2001 / 93409 files saved.\n",
      "3001 / 93409 files saved.\n",
      "4001 / 93409 files saved.\n",
      "5001 / 93409 files saved.\n",
      "6001 / 93409 files saved.\n",
      "7001 / 93409 files saved.\n",
      "8001 / 93409 files saved.\n",
      "9001 / 93409 files saved.\n",
      "10001 / 93409 files saved.\n",
      "11001 / 93409 files saved.\n",
      "12001 / 93409 files saved.\n",
      "13001 / 93409 files saved.\n",
      "14001 / 93409 files saved.\n",
      "15001 / 93409 files saved.\n",
      "16001 / 93409 files saved.\n",
      "17001 / 93409 files saved.\n",
      "18001 / 93409 files saved.\n",
      "19001 / 93409 files saved.\n",
      "20001 / 93409 files saved.\n",
      "21001 / 93409 files saved.\n",
      "22001 / 93409 files saved.\n",
      "23001 / 93409 files saved.\n",
      "24001 / 93409 files saved.\n",
      "25001 / 93409 files saved.\n",
      "26001 / 93409 files saved.\n",
      "27001 / 93409 files saved.\n",
      "28001 / 93409 files saved.\n",
      "29001 / 93409 files saved.\n",
      "30001 / 93409 files saved.\n",
      "31001 / 93409 files saved.\n",
      "32001 / 93409 files saved.\n",
      "33001 / 93409 files saved.\n",
      "34001 / 93409 files saved.\n",
      "35001 / 93409 files saved.\n",
      "36001 / 93409 files saved.\n",
      "37001 / 93409 files saved.\n",
      "38001 / 93409 files saved.\n",
      "39001 / 93409 files saved.\n",
      "40001 / 93409 files saved.\n",
      "41001 / 93409 files saved.\n",
      "42001 / 93409 files saved.\n",
      "43001 / 93409 files saved.\n",
      "44001 / 93409 files saved.\n",
      "45001 / 93409 files saved.\n",
      "46001 / 93409 files saved.\n",
      "47001 / 93409 files saved.\n",
      "48001 / 93409 files saved.\n",
      "49001 / 93409 files saved.\n",
      "50001 / 93409 files saved.\n",
      "51001 / 93409 files saved.\n",
      "52001 / 93409 files saved.\n",
      "53001 / 93409 files saved.\n",
      "54001 / 93409 files saved.\n",
      "55001 / 93409 files saved.\n",
      "56001 / 93409 files saved.\n",
      "57001 / 93409 files saved.\n",
      "58001 / 93409 files saved.\n",
      "59001 / 93409 files saved.\n",
      "60001 / 93409 files saved.\n",
      "61001 / 93409 files saved.\n",
      "62001 / 93409 files saved.\n",
      "63001 / 93409 files saved.\n",
      "64001 / 93409 files saved.\n",
      "65001 / 93409 files saved.\n",
      "66001 / 93409 files saved.\n",
      "67001 / 93409 files saved.\n",
      "68001 / 93409 files saved.\n",
      "69001 / 93409 files saved.\n",
      "70001 / 93409 files saved.\n",
      "71001 / 93409 files saved.\n",
      "72001 / 93409 files saved.\n",
      "73001 / 93409 files saved.\n",
      "74001 / 93409 files saved.\n",
      "75001 / 93409 files saved.\n",
      "76001 / 93409 files saved.\n",
      "77001 / 93409 files saved.\n",
      "78001 / 93409 files saved.\n",
      "79001 / 93409 files saved.\n",
      "80001 / 93409 files saved.\n",
      "81001 / 93409 files saved.\n",
      "82001 / 93409 files saved.\n",
      "83001 / 93409 files saved.\n",
      "84001 / 93409 files saved.\n",
      "85001 / 93409 files saved.\n",
      "86001 / 93409 files saved.\n",
      "87001 / 93409 files saved.\n",
      "88001 / 93409 files saved.\n",
      "89001 / 93409 files saved.\n",
      "90001 / 93409 files saved.\n",
      "91001 / 93409 files saved.\n",
      "92001 / 93409 files saved.\n",
      "93001 / 93409 files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in range(len(new_proc_texts)):\n",
    "    path_dir = all_texts.filenames[i].copy()\n",
    "    path_dir = re.sub('texts_txt','processed_texts_txt',path_dir)\n",
    "    path_folder = \"/\".join(path_dir.split(\"/\")[:-1])\n",
    "    if not os.path.isdir(path_folder) :\n",
    "        os.mkdir(path_folder)\n",
    "    text_file = open(path_dir, \"w\")\n",
    "    text_file.write(new_proc_texts[i])\n",
    "    text_file.close()\n",
    "    if i % 1000 == True:\n",
    "        print(i,\"/\",len(new_proc_texts),\"files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TFIDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train,text_test , y_train, y_test = train_test_split(\n",
    "    new_proc_texts, all_texts.target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81, 81, 42,  4, 70, 31, 21, 61, 44, 10])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF on text data ... \n",
      "Done ! \n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering \n",
    "print (\"TF-IDF on text data ... \")\n",
    "tfidf = TfidfVectorizer(binary=True)\n",
    "X_train = tfidf.fit_transform(text_train)\n",
    "#.astype('float16')\n",
    "X_test = tfidf.transform(text_test)\n",
    "#.astype('float16')\n",
    "print (\"Done ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62584, 60050)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, num_words = X_train.shape\n",
    "num_classes = len(all_texts.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size,_ = X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "clf = TruncatedSVD(1000)\n",
    "clf.fit(X_train)\n",
    "X_train_PCA = clf.transform(X_train)\n",
    "X_test_PCA = clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62584, 1000)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, num_words = X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on test data ... \n",
      "Accuracy :  0.8412327656123276\n",
      "CPU times: user 2min 51s, sys: 223 ms, total: 2min 52s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_PCA, y_train)\n",
    "print (\"Predict on test data ... \")\n",
    "y_pred = clf.predict(X_test_PCA)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62584 samples, validate on 30825 samples\n",
      "Epoch 1/50\n",
      "62584/62584 [==============================] - 2s 27us/step - loss: 4.5846 - acc: 0.0523 - val_loss: 4.5239 - val_acc: 0.1531\n",
      "Epoch 2/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 4.4047 - acc: 0.1545 - val_loss: 4.2064 - val_acc: 0.3341\n",
      "Epoch 3/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 3.9344 - acc: 0.2697 - val_loss: 3.5003 - val_acc: 0.5034\n",
      "Epoch 4/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 3.2067 - acc: 0.3637 - val_loss: 2.6703 - val_acc: 0.6070\n",
      "Epoch 5/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 2.5827 - acc: 0.4523 - val_loss: 2.1081 - val_acc: 0.6571\n",
      "Epoch 6/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 2.1765 - acc: 0.5158 - val_loss: 1.7654 - val_acc: 0.6922\n",
      "Epoch 7/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.9221 - acc: 0.5595 - val_loss: 1.5567 - val_acc: 0.7099\n",
      "Epoch 8/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.7402 - acc: 0.5955 - val_loss: 1.4164 - val_acc: 0.7239\n",
      "Epoch 9/50\n",
      "62584/62584 [==============================] - 1s 16us/step - loss: 1.6213 - acc: 0.6185 - val_loss: 1.3189 - val_acc: 0.7340\n",
      "Epoch 10/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.5204 - acc: 0.6425 - val_loss: 1.2426 - val_acc: 0.7432\n",
      "Epoch 11/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.4505 - acc: 0.6541 - val_loss: 1.1871 - val_acc: 0.7504\n",
      "Epoch 12/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.3788 - acc: 0.6712 - val_loss: 1.1382 - val_acc: 0.7573\n",
      "Epoch 13/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.3282 - acc: 0.6826 - val_loss: 1.1012 - val_acc: 0.7628\n",
      "Epoch 14/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.2873 - acc: 0.6898 - val_loss: 1.0717 - val_acc: 0.7664\n",
      "Epoch 15/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.2433 - acc: 0.7001 - val_loss: 1.0416 - val_acc: 0.7723\n",
      "Epoch 16/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.2144 - acc: 0.7040 - val_loss: 1.0206 - val_acc: 0.7755\n",
      "Epoch 17/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.1787 - acc: 0.7131 - val_loss: 1.0000 - val_acc: 0.7780\n",
      "Epoch 18/50\n",
      "62584/62584 [==============================] - 1s 16us/step - loss: 1.1480 - acc: 0.7207 - val_loss: 0.9793 - val_acc: 0.7807\n",
      "Epoch 19/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.1240 - acc: 0.7268 - val_loss: 0.9658 - val_acc: 0.7835\n",
      "Epoch 20/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.1012 - acc: 0.7311 - val_loss: 0.9512 - val_acc: 0.7854\n",
      "Epoch 21/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.0734 - acc: 0.7384 - val_loss: 0.9366 - val_acc: 0.7879\n",
      "Epoch 22/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.0541 - acc: 0.7414 - val_loss: 0.9258 - val_acc: 0.7895\n",
      "Epoch 23/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.0355 - acc: 0.7461 - val_loss: 0.9136 - val_acc: 0.7910\n",
      "Epoch 24/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.0184 - acc: 0.7509 - val_loss: 0.9044 - val_acc: 0.7941\n",
      "Epoch 25/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 1.0004 - acc: 0.7544 - val_loss: 0.8953 - val_acc: 0.7942\n",
      "Epoch 26/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.9870 - acc: 0.7555 - val_loss: 0.8873 - val_acc: 0.7968\n",
      "Epoch 27/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.9737 - acc: 0.7600 - val_loss: 0.8788 - val_acc: 0.7984\n",
      "Epoch 28/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.9565 - acc: 0.7644 - val_loss: 0.8717 - val_acc: 0.7996\n",
      "Epoch 29/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.9415 - acc: 0.7681 - val_loss: 0.8647 - val_acc: 0.7991\n",
      "Epoch 30/50\n",
      "62584/62584 [==============================] - 1s 16us/step - loss: 0.9276 - acc: 0.7703 - val_loss: 0.8577 - val_acc: 0.8011\n",
      "Epoch 31/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.9169 - acc: 0.7726 - val_loss: 0.8510 - val_acc: 0.8018\n",
      "Epoch 32/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8995 - acc: 0.7759 - val_loss: 0.8464 - val_acc: 0.8030\n",
      "Epoch 33/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8952 - acc: 0.7784 - val_loss: 0.8412 - val_acc: 0.8044\n",
      "Epoch 34/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8791 - acc: 0.7804 - val_loss: 0.8352 - val_acc: 0.8050\n",
      "Epoch 35/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8735 - acc: 0.7811 - val_loss: 0.8321 - val_acc: 0.8054\n",
      "Epoch 36/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8574 - acc: 0.7861 - val_loss: 0.8277 - val_acc: 0.8061\n",
      "Epoch 37/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8483 - acc: 0.7886 - val_loss: 0.8231 - val_acc: 0.8076\n",
      "Epoch 38/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8427 - acc: 0.7880 - val_loss: 0.8192 - val_acc: 0.8078\n",
      "Epoch 39/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8267 - acc: 0.7920 - val_loss: 0.8152 - val_acc: 0.8076\n",
      "Epoch 40/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8159 - acc: 0.7951 - val_loss: 0.8113 - val_acc: 0.8086\n",
      "Epoch 41/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8142 - acc: 0.7933 - val_loss: 0.8093 - val_acc: 0.8091\n",
      "Epoch 42/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.8051 - acc: 0.7973 - val_loss: 0.8053 - val_acc: 0.8101\n",
      "Epoch 43/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7915 - acc: 0.8006 - val_loss: 0.8020 - val_acc: 0.8112\n",
      "Epoch 44/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7845 - acc: 0.8027 - val_loss: 0.8000 - val_acc: 0.8103\n",
      "Epoch 45/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7788 - acc: 0.8024 - val_loss: 0.7966 - val_acc: 0.8111\n",
      "Epoch 46/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7742 - acc: 0.8043 - val_loss: 0.7945 - val_acc: 0.8116\n",
      "Epoch 47/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7595 - acc: 0.8073 - val_loss: 0.7916 - val_acc: 0.8130\n",
      "Epoch 48/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7539 - acc: 0.8096 - val_loss: 0.7901 - val_acc: 0.8123\n",
      "Epoch 49/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7473 - acc: 0.8101 - val_loss: 0.7877 - val_acc: 0.8133\n",
      "Epoch 50/50\n",
      "62584/62584 [==============================] - 1s 17us/step - loss: 0.7389 - acc: 0.8122 - val_loss: 0.7849 - val_acc: 0.8140\n",
      "Test loss: 0.7849118463449718\n",
      "Test accuracy: 0.8140145985411128\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "\n",
    "# Build neural network\n",
    "model = models.Sequential()\n",
    "model.add(Dense(256, input_dim=num_words, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              #loss='categorical_crossentropy',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_PCA, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_PCA, y_test))\n",
    "\n",
    "score = model.evaluate(X_test_PCA, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62584 samples, validate on 30825 samples\n",
      "Epoch 1/10\n",
      "62584/62584 [==============================] - 37s 595us/step - loss: 0.5223 - acc: 0.8709 - val_loss: 1.0235 - val_acc: 0.7813\n",
      "Epoch 2/10\n",
      "62584/62584 [==============================] - 37s 594us/step - loss: 0.4641 - acc: 0.8857 - val_loss: 1.0246 - val_acc: 0.7830\n",
      "Epoch 3/10\n",
      "62584/62584 [==============================] - 37s 593us/step - loss: 0.4174 - acc: 0.8989 - val_loss: 1.0356 - val_acc: 0.7854\n",
      "Epoch 4/10\n",
      "62584/62584 [==============================] - 37s 594us/step - loss: 0.3851 - acc: 0.9066 - val_loss: 1.0446 - val_acc: 0.7877\n",
      "Epoch 5/10\n",
      "62584/62584 [==============================] - 37s 594us/step - loss: 0.3522 - acc: 0.9151 - val_loss: 1.0560 - val_acc: 0.7856\n",
      "Epoch 6/10\n",
      "62584/62584 [==============================] - 37s 592us/step - loss: 0.3293 - acc: 0.9202 - val_loss: 1.0550 - val_acc: 0.7886\n",
      "Epoch 7/10\n",
      "62584/62584 [==============================] - 37s 593us/step - loss: 0.3048 - acc: 0.9259 - val_loss: 1.0688 - val_acc: 0.7884\n",
      "Epoch 8/10\n",
      "62584/62584 [==============================] - 37s 594us/step - loss: 0.2869 - acc: 0.9317 - val_loss: 1.0810 - val_acc: 0.7889\n",
      "Epoch 9/10\n",
      "62584/62584 [==============================] - 37s 593us/step - loss: 0.2708 - acc: 0.9352 - val_loss: 1.0983 - val_acc: 0.7886\n",
      "Epoch 10/10\n",
      "62584/62584 [==============================] - 37s 593us/step - loss: 0.2630 - acc: 0.9379 - val_loss: 1.1067 - val_acc: 0.7895\n",
      "Test loss: 1.106705611291593\n",
      "Test accuracy: 0.7894566099023007\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print('Using gpu: %s ' % use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(myNet2, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = myNet2(num_words, num_classes)\n",
    "if use_gpu:\n",
    "    net2 = net2.cuda()\n",
    "optimizer2 = optim.Adam(net2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myNet2(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.3)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.3)\n",
      "    (6): Linear(in_features=128, out_features=101, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "    def __init__(self, n_inputs, hidden_size, num_classes):\n",
    "        super(OurNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_inputs,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = OurNet(num_words, 100, num_classes)\n",
    "if use_gpu:\n",
    "    net3 = net3.cuda()\n",
    "#ouroptimizer = optim.SGD(net3.parameters(), lr=0.01)\n",
    "optimizer3 = optim.Adam(net3.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data,labels,batch_size=64,shuffle=True):\n",
    "    labels = np.array(labels)\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(len(labels))\n",
    "        data = data[index]\n",
    "        labels = labels[index]\n",
    "    for idx in range(0,len(labels),batch_size):\n",
    "        if type(data).__module__ == 'numpy' :\n",
    "            yield(data[idx:idx+batch_size],labels[idx:idx+batch_size])\n",
    "        else :\n",
    "            yield(data[idx:idx+batch_size].toarray(),labels[idx:idx+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_data=None,train_labels=None, \n",
    "                test_data = None, test_labels = None,\n",
    "                epochs=1, batch_size = 128,\n",
    "                optimizer=None,criterion=None ,\n",
    "                train=True,\n",
    "                shuffle=True ):\n",
    "    \n",
    "    if train:\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    if train :\n",
    "        for epoch in range(epochs):\n",
    "            #================TRAINING=====================#\n",
    "            batches = data_gen(data=train_data,labels=train_labels,batch_size = batch_size,shuffle=shuffle)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs,classes in batches:\n",
    "                if use_gpu:\n",
    "                    inputs , classes = torch.from_numpy(inputs).float().cuda(), torch.from_numpy(classes).long().cuda()\n",
    "                else:\n",
    "                    inputs , classes = torch.from_numpy(inputs).float(), torch.from_numpy(classes).long()\n",
    "\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)\n",
    "                \n",
    "                if optimizer is None:\n",
    "                    raise ValueError('Pass optimizer for train mode')\n",
    "                    \n",
    "                optimizer = optimizer\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                # statistics\n",
    "                #print(loss)\n",
    "                running_loss += loss.data.item()\n",
    "                running_corrects += torch.sum(preds == classes.data)\n",
    "                \n",
    "            train_size = len(train_labels)\n",
    "            \n",
    "            epoch_loss = running_loss / train_size\n",
    "            epoch_acc = running_corrects.data.item() / train_size\n",
    "            #print('Epoch: {:d} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            #             epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "            loss_history.append(epoch_loss)\n",
    "            acc_history.append(epoch_acc)\n",
    "            \n",
    "            #================VALIDATING===================#\n",
    "            batches = data_gen(data=test_data,labels=test_labels,shuffle=shuffle)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs,classes in batches:\n",
    "                if use_gpu:\n",
    "                    inputs , classes = torch.from_numpy(inputs).float().cuda(), torch.from_numpy(classes).long().cuda()\n",
    "                else:\n",
    "                    inputs , classes = torch.from_numpy(inputs).float(), torch.from_numpy(classes).long()\n",
    "\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs,classes)           \n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                # statistics\n",
    "                #print(loss)\n",
    "                running_loss += loss.data.item()\n",
    "                running_corrects += torch.sum(preds == classes.data)\n",
    "                \n",
    "            test_size = len(test_labels)    \n",
    "                \n",
    "            test_epoch_loss = running_loss / test_size\n",
    "            test_epoch_acc = running_corrects.data.item() / test_size\n",
    "            \n",
    "            val_loss_history.append(test_epoch_loss)\n",
    "            val_acc_history.append(test_epoch_acc)\n",
    "            \n",
    "            print('Epoch: {:d} Loss: {:.4f} Acc: {:.4f} -- Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "                     epoch, epoch_loss, epoch_acc, test_epoch_loss, test_epoch_acc))\n",
    "            \n",
    "        return loss_history,acc_history,val_loss_history,val_acc_history\n",
    "            \n",
    "    else :\n",
    "        batches = data_gen(data=test_data,labels=test_labels,shuffle=shuffle)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs,classes in batches:\n",
    "            if use_gpu:\n",
    "                inputs , classes = torch.from_numpy(inputs).float().cuda(), torch.from_numpy(classes).long().cuda()\n",
    "            else:\n",
    "                inputs , classes = torch.from_numpy(inputs).float(), torch.from_numpy(classes).long()\n",
    "\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,classes)           \n",
    "            _,preds = torch.max(outputs.data,1)\n",
    "            # statistics\n",
    "            #print(loss)\n",
    "            running_loss += loss.data.item()\n",
    "            running_corrects += torch.sum(preds == classes.data)\n",
    "\n",
    "        test_size = len(test_labels)    \n",
    "\n",
    "        test_epoch_loss = running_loss / test_size\n",
    "        test_epoch_acc = running_corrects.data.item() / test_size\n",
    "            \n",
    "        print('Test Loss: {:.4f} Test Acc: {:.4f}'.format(\n",
    "                     test_epoch_loss, test_epoch_acc))\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.0268 Acc: 0.2488 -- Test Loss: 0.0341 Test Acc: 0.4698\n",
      "Epoch: 1 Loss: 0.0140 Acc: 0.5654 -- Test Loss: 0.0255 Test Acc: 0.6026\n",
      "Epoch: 2 Loss: 0.0111 Acc: 0.6501 -- Test Loss: 0.0222 Test Acc: 0.6544\n",
      "Epoch: 3 Loss: 0.0097 Acc: 0.6943 -- Test Loss: 0.0205 Test Acc: 0.6777\n",
      "Epoch: 4 Loss: 0.0088 Acc: 0.7240 -- Test Loss: 0.0192 Test Acc: 0.6994\n",
      "Epoch: 5 Loss: 0.0080 Acc: 0.7433 -- Test Loss: 0.0185 Test Acc: 0.7101\n",
      "Epoch: 6 Loss: 0.0075 Acc: 0.7606 -- Test Loss: 0.0180 Test Acc: 0.7199\n",
      "Epoch: 7 Loss: 0.0071 Acc: 0.7721 -- Test Loss: 0.0174 Test Acc: 0.7300\n",
      "Epoch: 8 Loss: 0.0067 Acc: 0.7877 -- Test Loss: 0.0172 Test Acc: 0.7364\n",
      "Epoch: 9 Loss: 0.0064 Acc: 0.7933 -- Test Loss: 0.0169 Test Acc: 0.7370\n",
      "Epoch: 10 Loss: 0.0061 Acc: 0.8036 -- Test Loss: 0.0168 Test Acc: 0.7409\n",
      "Epoch: 11 Loss: 0.0058 Acc: 0.8126 -- Test Loss: 0.0167 Test Acc: 0.7472\n",
      "Epoch: 12 Loss: 0.0055 Acc: 0.8195 -- Test Loss: 0.0166 Test Acc: 0.7499\n",
      "Epoch: 13 Loss: 0.0053 Acc: 0.8258 -- Test Loss: 0.0164 Test Acc: 0.7533\n",
      "Epoch: 14 Loss: 0.0051 Acc: 0.8319 -- Test Loss: 0.0163 Test Acc: 0.7558\n",
      "Epoch: 15 Loss: 0.0049 Acc: 0.8382 -- Test Loss: 0.0161 Test Acc: 0.7599\n",
      "Epoch: 16 Loss: 0.0047 Acc: 0.8438 -- Test Loss: 0.0165 Test Acc: 0.7582\n",
      "Epoch: 17 Loss: 0.0045 Acc: 0.8498 -- Test Loss: 0.0163 Test Acc: 0.7571\n",
      "Epoch: 18 Loss: 0.0044 Acc: 0.8522 -- Test Loss: 0.0164 Test Acc: 0.7592\n",
      "Epoch: 19 Loss: 0.0043 Acc: 0.8562 -- Test Loss: 0.0165 Test Acc: 0.7616\n",
      "Epoch: 20 Loss: 0.0041 Acc: 0.8608 -- Test Loss: 0.0164 Test Acc: 0.7639\n",
      "Epoch: 21 Loss: 0.0040 Acc: 0.8661 -- Test Loss: 0.0167 Test Acc: 0.7589\n",
      "Epoch: 22 Loss: 0.0039 Acc: 0.8688 -- Test Loss: 0.0166 Test Acc: 0.7632\n",
      "Epoch: 23 Loss: 0.0037 Acc: 0.8706 -- Test Loss: 0.0167 Test Acc: 0.7640\n",
      "Epoch: 24 Loss: 0.0037 Acc: 0.8748 -- Test Loss: 0.0167 Test Acc: 0.7647\n",
      "Epoch: 25 Loss: 0.0036 Acc: 0.8778 -- Test Loss: 0.0170 Test Acc: 0.7655\n",
      "Epoch: 26 Loss: 0.0035 Acc: 0.8816 -- Test Loss: 0.0169 Test Acc: 0.7659\n",
      "Epoch: 27 Loss: 0.0034 Acc: 0.8813 -- Test Loss: 0.0171 Test Acc: 0.7630\n",
      "Epoch: 28 Loss: 0.0033 Acc: 0.8864 -- Test Loss: 0.0172 Test Acc: 0.7660\n",
      "Epoch: 29 Loss: 0.0032 Acc: 0.8869 -- Test Loss: 0.0174 Test Acc: 0.7636\n",
      "Epoch: 30 Loss: 0.0032 Acc: 0.8915 -- Test Loss: 0.0173 Test Acc: 0.7681\n",
      "Epoch: 31 Loss: 0.0031 Acc: 0.8929 -- Test Loss: 0.0175 Test Acc: 0.7644\n",
      "Epoch: 32 Loss: 0.0030 Acc: 0.8937 -- Test Loss: 0.0176 Test Acc: 0.7670\n",
      "Epoch: 33 Loss: 0.0030 Acc: 0.8955 -- Test Loss: 0.0177 Test Acc: 0.7666\n",
      "Epoch: 34 Loss: 0.0029 Acc: 0.8978 -- Test Loss: 0.0179 Test Acc: 0.7663\n",
      "Epoch: 35 Loss: 0.0029 Acc: 0.8990 -- Test Loss: 0.0179 Test Acc: 0.7643\n",
      "Epoch: 36 Loss: 0.0028 Acc: 0.9006 -- Test Loss: 0.0180 Test Acc: 0.7670\n",
      "Epoch: 37 Loss: 0.0028 Acc: 0.9038 -- Test Loss: 0.0182 Test Acc: 0.7659\n",
      "Epoch: 38 Loss: 0.0027 Acc: 0.9046 -- Test Loss: 0.0182 Test Acc: 0.7684\n",
      "Epoch: 39 Loss: 0.0027 Acc: 0.9057 -- Test Loss: 0.0183 Test Acc: 0.7691\n",
      "Epoch: 40 Loss: 0.0026 Acc: 0.9069 -- Test Loss: 0.0184 Test Acc: 0.7663\n",
      "Epoch: 41 Loss: 0.0026 Acc: 0.9075 -- Test Loss: 0.0183 Test Acc: 0.7685\n",
      "Epoch: 42 Loss: 0.0026 Acc: 0.9101 -- Test Loss: 0.0184 Test Acc: 0.7677\n",
      "Epoch: 43 Loss: 0.0025 Acc: 0.9111 -- Test Loss: 0.0186 Test Acc: 0.7713\n",
      "Epoch: 44 Loss: 0.0025 Acc: 0.9131 -- Test Loss: 0.0187 Test Acc: 0.7709\n",
      "Epoch: 45 Loss: 0.0025 Acc: 0.9120 -- Test Loss: 0.0189 Test Acc: 0.7688\n",
      "Epoch: 46 Loss: 0.0025 Acc: 0.9128 -- Test Loss: 0.0189 Test Acc: 0.7686\n",
      "Epoch: 47 Loss: 0.0024 Acc: 0.9156 -- Test Loss: 0.0188 Test Acc: 0.7716\n",
      "Epoch: 48 Loss: 0.0024 Acc: 0.9153 -- Test Loss: 0.0188 Test Acc: 0.7699\n",
      "Epoch: 49 Loss: 0.0024 Acc: 0.9171 -- Test Loss: 0.0191 Test Acc: 0.7695\n",
      "Epoch: 50 Loss: 0.0024 Acc: 0.9169 -- Test Loss: 0.0191 Test Acc: 0.7709\n",
      "Epoch: 51 Loss: 0.0023 Acc: 0.9181 -- Test Loss: 0.0191 Test Acc: 0.7713\n",
      "Epoch: 52 Loss: 0.0023 Acc: 0.9198 -- Test Loss: 0.0196 Test Acc: 0.7705\n",
      "Epoch: 53 Loss: 0.0023 Acc: 0.9225 -- Test Loss: 0.0197 Test Acc: 0.7683\n",
      "Epoch: 54 Loss: 0.0023 Acc: 0.9199 -- Test Loss: 0.0195 Test Acc: 0.7703\n",
      "Epoch: 55 Loss: 0.0022 Acc: 0.9222 -- Test Loss: 0.0195 Test Acc: 0.7719\n",
      "Epoch: 56 Loss: 0.0022 Acc: 0.9220 -- Test Loss: 0.0197 Test Acc: 0.7715\n",
      "Epoch: 57 Loss: 0.0022 Acc: 0.9232 -- Test Loss: 0.0199 Test Acc: 0.7674\n",
      "Epoch: 58 Loss: 0.0022 Acc: 0.9232 -- Test Loss: 0.0200 Test Acc: 0.7689\n",
      "Epoch: 59 Loss: 0.0022 Acc: 0.9222 -- Test Loss: 0.0199 Test Acc: 0.7695\n",
      "Epoch: 60 Loss: 0.0022 Acc: 0.9244 -- Test Loss: 0.0200 Test Acc: 0.7711\n",
      "Epoch: 61 Loss: 0.0021 Acc: 0.9241 -- Test Loss: 0.0201 Test Acc: 0.7700\n",
      "Epoch: 62 Loss: 0.0021 Acc: 0.9263 -- Test Loss: 0.0202 Test Acc: 0.7705\n",
      "Epoch: 63 Loss: 0.0021 Acc: 0.9264 -- Test Loss: 0.0202 Test Acc: 0.7690\n",
      "Epoch: 64 Loss: 0.0021 Acc: 0.9265 -- Test Loss: 0.0204 Test Acc: 0.7721\n",
      "Epoch: 65 Loss: 0.0021 Acc: 0.9249 -- Test Loss: 0.0201 Test Acc: 0.7702\n",
      "Epoch: 66 Loss: 0.0020 Acc: 0.9297 -- Test Loss: 0.0205 Test Acc: 0.7708\n",
      "Epoch: 67 Loss: 0.0020 Acc: 0.9294 -- Test Loss: 0.0206 Test Acc: 0.7705\n",
      "Epoch: 68 Loss: 0.0021 Acc: 0.9295 -- Test Loss: 0.0206 Test Acc: 0.7715\n",
      "Epoch: 69 Loss: 0.0020 Acc: 0.9297 -- Test Loss: 0.0207 Test Acc: 0.7702\n",
      "Epoch: 70 Loss: 0.0020 Acc: 0.9285 -- Test Loss: 0.0207 Test Acc: 0.7689\n",
      "Epoch: 71 Loss: 0.0020 Acc: 0.9296 -- Test Loss: 0.0209 Test Acc: 0.7706\n",
      "Epoch: 72 Loss: 0.0020 Acc: 0.9318 -- Test Loss: 0.0211 Test Acc: 0.7698\n",
      "Epoch: 73 Loss: 0.0020 Acc: 0.9312 -- Test Loss: 0.0211 Test Acc: 0.7707\n",
      "Epoch: 74 Loss: 0.0020 Acc: 0.9308 -- Test Loss: 0.0212 Test Acc: 0.7697\n",
      "Epoch: 75 Loss: 0.0020 Acc: 0.9321 -- Test Loss: 0.0209 Test Acc: 0.7704\n",
      "Epoch: 76 Loss: 0.0019 Acc: 0.9336 -- Test Loss: 0.0211 Test Acc: 0.7715\n",
      "Epoch: 77 Loss: 0.0019 Acc: 0.9330 -- Test Loss: 0.0212 Test Acc: 0.7717\n",
      "Epoch: 78 Loss: 0.0020 Acc: 0.9325 -- Test Loss: 0.0210 Test Acc: 0.7728\n",
      "Epoch: 79 Loss: 0.0019 Acc: 0.9338 -- Test Loss: 0.0213 Test Acc: 0.7698\n",
      "Epoch: 80 Loss: 0.0019 Acc: 0.9338 -- Test Loss: 0.0214 Test Acc: 0.7716\n",
      "Epoch: 81 Loss: 0.0019 Acc: 0.9347 -- Test Loss: 0.0213 Test Acc: 0.7711\n",
      "Epoch: 82 Loss: 0.0019 Acc: 0.9343 -- Test Loss: 0.0215 Test Acc: 0.7711\n",
      "Epoch: 83 Loss: 0.0019 Acc: 0.9361 -- Test Loss: 0.0216 Test Acc: 0.7691\n",
      "Epoch: 84 Loss: 0.0019 Acc: 0.9341 -- Test Loss: 0.0218 Test Acc: 0.7695\n",
      "Epoch: 85 Loss: 0.0019 Acc: 0.9351 -- Test Loss: 0.0219 Test Acc: 0.7689\n",
      "Epoch: 86 Loss: 0.0019 Acc: 0.9349 -- Test Loss: 0.0221 Test Acc: 0.7681\n",
      "Epoch: 87 Loss: 0.0018 Acc: 0.9371 -- Test Loss: 0.0218 Test Acc: 0.7709\n",
      "Epoch: 88 Loss: 0.0018 Acc: 0.9373 -- Test Loss: 0.0220 Test Acc: 0.7691\n",
      "Epoch: 89 Loss: 0.0019 Acc: 0.9358 -- Test Loss: 0.0219 Test Acc: 0.7719\n",
      "Epoch: 90 Loss: 0.0018 Acc: 0.9370 -- Test Loss: 0.0221 Test Acc: 0.7725\n",
      "Epoch: 91 Loss: 0.0018 Acc: 0.9367 -- Test Loss: 0.0220 Test Acc: 0.7704\n",
      "Epoch: 92 Loss: 0.0018 Acc: 0.9379 -- Test Loss: 0.0224 Test Acc: 0.7706\n",
      "Epoch: 93 Loss: 0.0018 Acc: 0.9385 -- Test Loss: 0.0220 Test Acc: 0.7725\n",
      "Epoch: 94 Loss: 0.0018 Acc: 0.9382 -- Test Loss: 0.0224 Test Acc: 0.7714\n",
      "Epoch: 95 Loss: 0.0018 Acc: 0.9385 -- Test Loss: 0.0225 Test Acc: 0.7716\n",
      "Epoch: 96 Loss: 0.0018 Acc: 0.9399 -- Test Loss: 0.0224 Test Acc: 0.7739\n",
      "Epoch: 97 Loss: 0.0018 Acc: 0.9389 -- Test Loss: 0.0224 Test Acc: 0.7711\n",
      "Epoch: 98 Loss: 0.0018 Acc: 0.9385 -- Test Loss: 0.0225 Test Acc: 0.7718\n",
      "Epoch: 99 Loss: 0.0018 Acc: 0.9392 -- Test Loss: 0.0223 Test Acc: 0.7727\n",
      "DONE !\n",
      "CPU times: user 2min 45s, sys: 19.8 s, total: 3min 4s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train_model(net2,\n",
    "                train_data=X_train_PCA,train_labels=y_train, \n",
    "                test_data = X_test_PCA, test_labels = y_test,\n",
    "                epochs=100, batch_size = 128,\n",
    "                optimizer=optimizer2,criterion=criterion ,\n",
    "                train=True,\n",
    "                shuffle=True )\n",
    "print(\"DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy history')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[1], color ='red')\n",
    "plt.plot(history[3], color ='orange')\n",
    "plt.plot()\n",
    "plt.title(\"Accuracy history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss history')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[0], color ='red')\n",
    "plt.plot(history[2], color ='orange')\n",
    "plt.plot()\n",
    "plt.title(\"Loss history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.0331 Acc: 0.0900 -- Test Loss: 0.0525 Test Acc: 0.2117\n",
      "Epoch: 1 Loss: 0.0222 Acc: 0.3079 -- Test Loss: 0.0399 Test Acc: 0.3698\n",
      "Epoch: 2 Loss: 0.0182 Acc: 0.4142 -- Test Loss: 0.0353 Test Acc: 0.4316\n",
      "Epoch: 3 Loss: 0.0164 Acc: 0.4683 -- Test Loss: 0.0330 Test Acc: 0.4678\n",
      "Epoch: 4 Loss: 0.0153 Acc: 0.5016 -- Test Loss: 0.0314 Test Acc: 0.4967\n",
      "Epoch: 5 Loss: 0.0145 Acc: 0.5241 -- Test Loss: 0.0302 Test Acc: 0.5163\n",
      "Epoch: 6 Loss: 0.0140 Acc: 0.5437 -- Test Loss: 0.0295 Test Acc: 0.5264\n",
      "Epoch: 7 Loss: 0.0135 Acc: 0.5587 -- Test Loss: 0.0288 Test Acc: 0.5426\n",
      "Epoch: 8 Loss: 0.0131 Acc: 0.5721 -- Test Loss: 0.0282 Test Acc: 0.5506\n",
      "Epoch: 9 Loss: 0.0128 Acc: 0.5814 -- Test Loss: 0.0279 Test Acc: 0.5536\n",
      "Epoch: 10 Loss: 0.0125 Acc: 0.5923 -- Test Loss: 0.0276 Test Acc: 0.5649\n",
      "Epoch: 11 Loss: 0.0123 Acc: 0.5977 -- Test Loss: 0.0274 Test Acc: 0.5656\n",
      "Epoch: 12 Loss: 0.0120 Acc: 0.6045 -- Test Loss: 0.0271 Test Acc: 0.5699\n",
      "Epoch: 13 Loss: 0.0118 Acc: 0.6109 -- Test Loss: 0.0271 Test Acc: 0.5745\n",
      "Epoch: 14 Loss: 0.0117 Acc: 0.6190 -- Test Loss: 0.0266 Test Acc: 0.5813\n",
      "Epoch: 15 Loss: 0.0115 Acc: 0.6243 -- Test Loss: 0.0265 Test Acc: 0.5845\n",
      "Epoch: 16 Loss: 0.0114 Acc: 0.6227 -- Test Loss: 0.0264 Test Acc: 0.5874\n",
      "Epoch: 17 Loss: 0.0113 Acc: 0.6278 -- Test Loss: 0.0262 Test Acc: 0.5895\n",
      "Epoch: 18 Loss: 0.0111 Acc: 0.6321 -- Test Loss: 0.0262 Test Acc: 0.5902\n",
      "Epoch: 19 Loss: 0.0110 Acc: 0.6366 -- Test Loss: 0.0263 Test Acc: 0.5930\n",
      "Epoch: 20 Loss: 0.0110 Acc: 0.6384 -- Test Loss: 0.0261 Test Acc: 0.5924\n",
      "Epoch: 21 Loss: 0.0109 Acc: 0.6410 -- Test Loss: 0.0261 Test Acc: 0.5956\n",
      "Epoch: 22 Loss: 0.0108 Acc: 0.6428 -- Test Loss: 0.0257 Test Acc: 0.6001\n",
      "Epoch: 23 Loss: 0.0106 Acc: 0.6494 -- Test Loss: 0.0261 Test Acc: 0.5978\n",
      "Epoch: 24 Loss: 0.0106 Acc: 0.6495 -- Test Loss: 0.0258 Test Acc: 0.6011\n",
      "Epoch: 25 Loss: 0.0106 Acc: 0.6491 -- Test Loss: 0.0258 Test Acc: 0.6001\n",
      "Epoch: 26 Loss: 0.0105 Acc: 0.6539 -- Test Loss: 0.0257 Test Acc: 0.6032\n",
      "Epoch: 27 Loss: 0.0103 Acc: 0.6553 -- Test Loss: 0.0257 Test Acc: 0.6042\n",
      "Epoch: 28 Loss: 0.0104 Acc: 0.6569 -- Test Loss: 0.0258 Test Acc: 0.6024\n",
      "Epoch: 29 Loss: 0.0102 Acc: 0.6608 -- Test Loss: 0.0257 Test Acc: 0.6057\n",
      "Epoch: 30 Loss: 0.0102 Acc: 0.6606 -- Test Loss: 0.0259 Test Acc: 0.6031\n",
      "Epoch: 31 Loss: 0.0101 Acc: 0.6630 -- Test Loss: 0.0258 Test Acc: 0.6041\n",
      "Epoch: 32 Loss: 0.0101 Acc: 0.6635 -- Test Loss: 0.0257 Test Acc: 0.6105\n",
      "Epoch: 33 Loss: 0.0100 Acc: 0.6665 -- Test Loss: 0.0258 Test Acc: 0.6091\n",
      "Epoch: 34 Loss: 0.0099 Acc: 0.6699 -- Test Loss: 0.0258 Test Acc: 0.6078\n",
      "Epoch: 35 Loss: 0.0099 Acc: 0.6676 -- Test Loss: 0.0255 Test Acc: 0.6123\n",
      "Epoch: 36 Loss: 0.0099 Acc: 0.6694 -- Test Loss: 0.0260 Test Acc: 0.6079\n",
      "Epoch: 37 Loss: 0.0098 Acc: 0.6705 -- Test Loss: 0.0256 Test Acc: 0.6098\n",
      "Epoch: 38 Loss: 0.0098 Acc: 0.6714 -- Test Loss: 0.0258 Test Acc: 0.6098\n",
      "Epoch: 39 Loss: 0.0097 Acc: 0.6758 -- Test Loss: 0.0258 Test Acc: 0.6103\n",
      "Epoch: 40 Loss: 0.0097 Acc: 0.6744 -- Test Loss: 0.0257 Test Acc: 0.6090\n",
      "Epoch: 41 Loss: 0.0096 Acc: 0.6774 -- Test Loss: 0.0258 Test Acc: 0.6133\n",
      "Epoch: 42 Loss: 0.0096 Acc: 0.6777 -- Test Loss: 0.0257 Test Acc: 0.6150\n",
      "Epoch: 43 Loss: 0.0096 Acc: 0.6767 -- Test Loss: 0.0257 Test Acc: 0.6166\n",
      "Epoch: 44 Loss: 0.0096 Acc: 0.6782 -- Test Loss: 0.0257 Test Acc: 0.6161\n",
      "Epoch: 45 Loss: 0.0095 Acc: 0.6807 -- Test Loss: 0.0257 Test Acc: 0.6158\n",
      "Epoch: 46 Loss: 0.0095 Acc: 0.6798 -- Test Loss: 0.0259 Test Acc: 0.6155\n",
      "Epoch: 47 Loss: 0.0095 Acc: 0.6794 -- Test Loss: 0.0260 Test Acc: 0.6157\n",
      "Epoch: 48 Loss: 0.0095 Acc: 0.6816 -- Test Loss: 0.0257 Test Acc: 0.6132\n",
      "Epoch: 49 Loss: 0.0094 Acc: 0.6826 -- Test Loss: 0.0260 Test Acc: 0.6130\n",
      "Epoch: 50 Loss: 0.0094 Acc: 0.6822 -- Test Loss: 0.0258 Test Acc: 0.6166\n",
      "Epoch: 51 Loss: 0.0094 Acc: 0.6843 -- Test Loss: 0.0258 Test Acc: 0.6169\n",
      "Epoch: 52 Loss: 0.0093 Acc: 0.6860 -- Test Loss: 0.0259 Test Acc: 0.6172\n",
      "Epoch: 53 Loss: 0.0093 Acc: 0.6854 -- Test Loss: 0.0260 Test Acc: 0.6169\n",
      "Epoch: 54 Loss: 0.0093 Acc: 0.6852 -- Test Loss: 0.0258 Test Acc: 0.6183\n",
      "Epoch: 55 Loss: 0.0093 Acc: 0.6855 -- Test Loss: 0.0259 Test Acc: 0.6143\n",
      "Epoch: 56 Loss: 0.0093 Acc: 0.6874 -- Test Loss: 0.0262 Test Acc: 0.6148\n",
      "Epoch: 57 Loss: 0.0092 Acc: 0.6884 -- Test Loss: 0.0259 Test Acc: 0.6178\n",
      "Epoch: 58 Loss: 0.0092 Acc: 0.6885 -- Test Loss: 0.0260 Test Acc: 0.6175\n",
      "Epoch: 59 Loss: 0.0092 Acc: 0.6875 -- Test Loss: 0.0261 Test Acc: 0.6159\n",
      "Epoch: 60 Loss: 0.0091 Acc: 0.6937 -- Test Loss: 0.0262 Test Acc: 0.6182\n",
      "Epoch: 61 Loss: 0.0091 Acc: 0.6912 -- Test Loss: 0.0261 Test Acc: 0.6199\n",
      "Epoch: 62 Loss: 0.0091 Acc: 0.6935 -- Test Loss: 0.0263 Test Acc: 0.6177\n",
      "Epoch: 63 Loss: 0.0091 Acc: 0.6930 -- Test Loss: 0.0260 Test Acc: 0.6201\n",
      "Epoch: 64 Loss: 0.0091 Acc: 0.6947 -- Test Loss: 0.0262 Test Acc: 0.6162\n",
      "Epoch: 65 Loss: 0.0091 Acc: 0.6922 -- Test Loss: 0.0260 Test Acc: 0.6193\n",
      "Epoch: 66 Loss: 0.0091 Acc: 0.6924 -- Test Loss: 0.0260 Test Acc: 0.6210\n",
      "Epoch: 67 Loss: 0.0090 Acc: 0.6933 -- Test Loss: 0.0260 Test Acc: 0.6194\n",
      "Epoch: 68 Loss: 0.0090 Acc: 0.6934 -- Test Loss: 0.0264 Test Acc: 0.6212\n",
      "Epoch: 69 Loss: 0.0090 Acc: 0.6970 -- Test Loss: 0.0260 Test Acc: 0.6220\n",
      "Epoch: 70 Loss: 0.0090 Acc: 0.6950 -- Test Loss: 0.0263 Test Acc: 0.6214\n",
      "Epoch: 71 Loss: 0.0090 Acc: 0.6957 -- Test Loss: 0.0263 Test Acc: 0.6191\n",
      "Epoch: 72 Loss: 0.0090 Acc: 0.6939 -- Test Loss: 0.0264 Test Acc: 0.6193\n",
      "Epoch: 73 Loss: 0.0089 Acc: 0.6986 -- Test Loss: 0.0262 Test Acc: 0.6239\n",
      "Epoch: 74 Loss: 0.0088 Acc: 0.6989 -- Test Loss: 0.0262 Test Acc: 0.6186\n",
      "Epoch: 75 Loss: 0.0089 Acc: 0.6979 -- Test Loss: 0.0262 Test Acc: 0.6172\n",
      "Epoch: 76 Loss: 0.0089 Acc: 0.6968 -- Test Loss: 0.0266 Test Acc: 0.6143\n",
      "Epoch: 77 Loss: 0.0088 Acc: 0.7002 -- Test Loss: 0.0262 Test Acc: 0.6189\n",
      "Epoch: 78 Loss: 0.0088 Acc: 0.6990 -- Test Loss: 0.0264 Test Acc: 0.6241\n",
      "Epoch: 79 Loss: 0.0088 Acc: 0.6979 -- Test Loss: 0.0266 Test Acc: 0.6199\n",
      "Epoch: 80 Loss: 0.0088 Acc: 0.6983 -- Test Loss: 0.0265 Test Acc: 0.6232\n",
      "Epoch: 81 Loss: 0.0088 Acc: 0.6994 -- Test Loss: 0.0267 Test Acc: 0.6218\n",
      "Epoch: 82 Loss: 0.0088 Acc: 0.6998 -- Test Loss: 0.0263 Test Acc: 0.6267\n",
      "Epoch: 83 Loss: 0.0087 Acc: 0.7013 -- Test Loss: 0.0266 Test Acc: 0.6203\n",
      "Epoch: 84 Loss: 0.0087 Acc: 0.7039 -- Test Loss: 0.0265 Test Acc: 0.6250\n",
      "Epoch: 85 Loss: 0.0088 Acc: 0.6984 -- Test Loss: 0.0265 Test Acc: 0.6208\n",
      "Epoch: 86 Loss: 0.0087 Acc: 0.7044 -- Test Loss: 0.0267 Test Acc: 0.6190\n",
      "Epoch: 87 Loss: 0.0086 Acc: 0.7059 -- Test Loss: 0.0267 Test Acc: 0.6230\n",
      "Epoch: 88 Loss: 0.0087 Acc: 0.7027 -- Test Loss: 0.0267 Test Acc: 0.6248\n",
      "Epoch: 89 Loss: 0.0087 Acc: 0.7035 -- Test Loss: 0.0268 Test Acc: 0.6227\n",
      "Epoch: 90 Loss: 0.0086 Acc: 0.7052 -- Test Loss: 0.0267 Test Acc: 0.6224\n",
      "Epoch: 91 Loss: 0.0086 Acc: 0.7059 -- Test Loss: 0.0265 Test Acc: 0.6238\n",
      "Epoch: 92 Loss: 0.0086 Acc: 0.7054 -- Test Loss: 0.0265 Test Acc: 0.6221\n",
      "Epoch: 93 Loss: 0.0086 Acc: 0.7051 -- Test Loss: 0.0269 Test Acc: 0.6205\n",
      "Epoch: 94 Loss: 0.0086 Acc: 0.7069 -- Test Loss: 0.0269 Test Acc: 0.6236\n",
      "Epoch: 95 Loss: 0.0086 Acc: 0.7063 -- Test Loss: 0.0270 Test Acc: 0.6225\n",
      "Epoch: 96 Loss: 0.0086 Acc: 0.7070 -- Test Loss: 0.0269 Test Acc: 0.6220\n",
      "Epoch: 97 Loss: 0.0086 Acc: 0.7065 -- Test Loss: 0.0270 Test Acc: 0.6215\n",
      "Epoch: 98 Loss: 0.0086 Acc: 0.7070 -- Test Loss: 0.0267 Test Acc: 0.6271\n",
      "Epoch: 99 Loss: 0.0085 Acc: 0.7054 -- Test Loss: 0.0270 Test Acc: 0.6215\n",
      "CPU times: user 2min 50s, sys: 19.7 s, total: 3min 10s\n",
      "Wall time: 3min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.03306124950710898,\n",
       "  0.022213012387681523,\n",
       "  0.018237325879416014,\n",
       "  0.016424402813617033,\n",
       "  0.015313011262326323,\n",
       "  0.014549141457116499,\n",
       "  0.013970562496042673,\n",
       "  0.013522375634559871,\n",
       "  0.013072579965770633,\n",
       "  0.012808084889825433,\n",
       "  0.012510158213968094,\n",
       "  0.012280358481358403,\n",
       "  0.012036213401057875,\n",
       "  0.011846882414592284,\n",
       "  0.011654061615413738,\n",
       "  0.011518225542392386,\n",
       "  0.011400882974238434,\n",
       "  0.011258829469329086,\n",
       "  0.011100656796633915,\n",
       "  0.011005446363383816,\n",
       "  0.010950213691302307,\n",
       "  0.010861382996024157,\n",
       "  0.010839667290765338,\n",
       "  0.010615501838062298,\n",
       "  0.010596997693998193,\n",
       "  0.010551117048206559,\n",
       "  0.01046054905048801,\n",
       "  0.010324542570316805,\n",
       "  0.010360062858545764,\n",
       "  0.010249424098619305,\n",
       "  0.01017632954194999,\n",
       "  0.010101861848895787,\n",
       "  0.010102541786042928,\n",
       "  0.010031870203327221,\n",
       "  0.009943539317910147,\n",
       "  0.009948701602631208,\n",
       "  0.009918034070023913,\n",
       "  0.009803630017333668,\n",
       "  0.009781654329384042,\n",
       "  0.009746033767368975,\n",
       "  0.009726945153209814,\n",
       "  0.009643236549708539,\n",
       "  0.009648992997041432,\n",
       "  0.009634469821144366,\n",
       "  0.009579910178494152,\n",
       "  0.009527130314117652,\n",
       "  0.009532052624047329,\n",
       "  0.00949120520878734,\n",
       "  0.009507195195715308,\n",
       "  0.009411607093363333,\n",
       "  0.009435069975486637,\n",
       "  0.009421990507439582,\n",
       "  0.009331528985498811,\n",
       "  0.00932886817583141,\n",
       "  0.009304644881147536,\n",
       "  0.009297733895731113,\n",
       "  0.009297460301420991,\n",
       "  0.00920899587889454,\n",
       "  0.009207013292064543,\n",
       "  0.009233291705071004,\n",
       "  0.009119363002851462,\n",
       "  0.009145859108432302,\n",
       "  0.009063765289464083,\n",
       "  0.009083643378495834,\n",
       "  0.00905466259809676,\n",
       "  0.009051545819182629,\n",
       "  0.009056706138161858,\n",
       "  0.008983306315310026,\n",
       "  0.0090139894862317,\n",
       "  0.008978093185114553,\n",
       "  0.009003595176764679,\n",
       "  0.008959819375706743,\n",
       "  0.008987513286644093,\n",
       "  0.00893997859386445,\n",
       "  0.008849758279113289,\n",
       "  0.00885029724952313,\n",
       "  0.008922740173251462,\n",
       "  0.008814270292289358,\n",
       "  0.008816754398528198,\n",
       "  0.008835935605413452,\n",
       "  0.008814775869850307,\n",
       "  0.008784953604326,\n",
       "  0.008791512821003496,\n",
       "  0.008749105900080326,\n",
       "  0.008667329168412655,\n",
       "  0.00880592078480988,\n",
       "  0.008724109493610303,\n",
       "  0.008587353961037717,\n",
       "  0.008720549269434467,\n",
       "  0.008657416693316721,\n",
       "  0.008630563725592245,\n",
       "  0.008586603633380372,\n",
       "  0.00861783464912196,\n",
       "  0.00860538304701617,\n",
       "  0.008605274239777147,\n",
       "  0.008605335260632244,\n",
       "  0.00857049065615749,\n",
       "  0.008589386273326432,\n",
       "  0.008562954901033665,\n",
       "  0.008525268632928596],\n",
       " [0.09003898760066471,\n",
       "  0.3078901955771443,\n",
       "  0.41416336443819507,\n",
       "  0.4683145851974946,\n",
       "  0.5015658954365333,\n",
       "  0.5241115940176403,\n",
       "  0.5436852869743065,\n",
       "  0.5586891218202735,\n",
       "  0.5720791256551195,\n",
       "  0.5813946056500064,\n",
       "  0.5922919596062891,\n",
       "  0.5977406365844306,\n",
       "  0.6044675955515787,\n",
       "  0.6109229195960629,\n",
       "  0.6190240317013933,\n",
       "  0.6243448804806341,\n",
       "  0.6227470279943755,\n",
       "  0.6277962418509523,\n",
       "  0.6320944650389876,\n",
       "  0.6366004090502365,\n",
       "  0.6384219608845711,\n",
       "  0.6409625463377221,\n",
       "  0.642816055221782,\n",
       "  0.6493672504154416,\n",
       "  0.6494791000894797,\n",
       "  0.6491115940176403,\n",
       "  0.6538891729515531,\n",
       "  0.6552952831394606,\n",
       "  0.656893135625719,\n",
       "  0.6608078742170522,\n",
       "  0.6605841748689761,\n",
       "  0.6630288891729516,\n",
       "  0.6635242234436917,\n",
       "  0.6665441646427201,\n",
       "  0.6699316119135882,\n",
       "  0.6676307043333759,\n",
       "  0.6693723635433977,\n",
       "  0.6704589032340534,\n",
       "  0.6713696791512208,\n",
       "  0.6757797520132941,\n",
       "  0.6743736418253867,\n",
       "  0.67736162597469,\n",
       "  0.6776652179470791,\n",
       "  0.6766745494055989,\n",
       "  0.6782244663172696,\n",
       "  0.6806851591461076,\n",
       "  0.6798223188035281,\n",
       "  0.6793589415825131,\n",
       "  0.6816119135881376,\n",
       "  0.6825706250798926,\n",
       "  0.6822031190080532,\n",
       "  0.6842643487153266,\n",
       "  0.6859580723507606,\n",
       "  0.6854148025054327,\n",
       "  0.6851591461076313,\n",
       "  0.6855266521794708,\n",
       "  0.687444075162981,\n",
       "  0.6884027866547361,\n",
       "  0.6885306148536368,\n",
       "  0.6874600536878436,\n",
       "  0.6937236354339767,\n",
       "  0.6911990285056884,\n",
       "  0.6934679790361754,\n",
       "  0.6929566662405726,\n",
       "  0.6946823469257318,\n",
       "  0.6922056755720312,\n",
       "  0.6924453534449699,\n",
       "  0.6933401508372747,\n",
       "  0.6933721078869999,\n",
       "  0.696983254505944,\n",
       "  0.6949539818483957,\n",
       "  0.6957369295666624,\n",
       "  0.6938514636328774,\n",
       "  0.6986450210916528,\n",
       "  0.6988527419148665,\n",
       "  0.6978940304231114,\n",
       "  0.696791512207593,\n",
       "  0.7001629809535984,\n",
       "  0.6989645915889046,\n",
       "  0.6978620733733861,\n",
       "  0.6982615364949508,\n",
       "  0.6994119902850569,\n",
       "  0.6997794963568963,\n",
       "  0.701329413268567,\n",
       "  0.7039179342963058,\n",
       "  0.6984053432187141,\n",
       "  0.7043653329924582,\n",
       "  0.7058992713792662,\n",
       "  0.7026716093570241,\n",
       "  0.7034705356001534,\n",
       "  0.7051802377604499,\n",
       "  0.7058992713792662,\n",
       "  0.7054199156333887,\n",
       "  0.7051323021858622,\n",
       "  0.7069059184456091,\n",
       "  0.7062507989262431,\n",
       "  0.7070497251693724,\n",
       "  0.7065064553240444,\n",
       "  0.706985811069922,\n",
       "  0.705371980058801],\n",
       " [0.052530799828596646,\n",
       "  0.03987036591318692,\n",
       "  0.035325165564513074,\n",
       "  0.03303104033241983,\n",
       "  0.03135390627026461,\n",
       "  0.03021618736728071,\n",
       "  0.02950453322782617,\n",
       "  0.028811767984964164,\n",
       "  0.02818303332905023,\n",
       "  0.027949866374527062,\n",
       "  0.027616932552612997,\n",
       "  0.027432480794283207,\n",
       "  0.027145763723616278,\n",
       "  0.0270921041012197,\n",
       "  0.026615785585606087,\n",
       "  0.02651895183136283,\n",
       "  0.02640426715408412,\n",
       "  0.026209327335388603,\n",
       "  0.02620478610536091,\n",
       "  0.026296055396204544,\n",
       "  0.026085438840484,\n",
       "  0.02611580355227332,\n",
       "  0.025712781633097082,\n",
       "  0.026056313769950774,\n",
       "  0.025808952761585888,\n",
       "  0.02584170922359024,\n",
       "  0.02568697147029063,\n",
       "  0.025692045978676085,\n",
       "  0.02577924124322475,\n",
       "  0.025697036802720365,\n",
       "  0.025898621975650463,\n",
       "  0.025803992079218907,\n",
       "  0.025689985259025928,\n",
       "  0.02582855585814489,\n",
       "  0.025750229708663062,\n",
       "  0.0254798563302095,\n",
       "  0.025981151065687193,\n",
       "  0.025636191134131833,\n",
       "  0.025842697765415312,\n",
       "  0.025758783726409887,\n",
       "  0.025725141434000454,\n",
       "  0.025753717196248745,\n",
       "  0.025670718810274072,\n",
       "  0.025650829207288086,\n",
       "  0.02572500789252511,\n",
       "  0.025698549414879892,\n",
       "  0.025863228464938718,\n",
       "  0.026034251531738627,\n",
       "  0.025742217848577446,\n",
       "  0.02604729251753288,\n",
       "  0.025777962726011378,\n",
       "  0.025783990264809046,\n",
       "  0.02593244633825446,\n",
       "  0.025980557616126315,\n",
       "  0.025845043829371073,\n",
       "  0.025914009866536584,\n",
       "  0.026204152461096027,\n",
       "  0.02590923489928729,\n",
       "  0.025995128469938972,\n",
       "  0.026077779603990617,\n",
       "  0.026244227932607462,\n",
       "  0.02611523276311251,\n",
       "  0.026293227262643906,\n",
       "  0.02600098447884763,\n",
       "  0.02619670292080077,\n",
       "  0.02601434151138219,\n",
       "  0.025993505170068796,\n",
       "  0.026041374345765495,\n",
       "  0.0264304698838128,\n",
       "  0.026016114987886155,\n",
       "  0.026346403846678846,\n",
       "  0.026281215208676996,\n",
       "  0.026352907487849152,\n",
       "  0.026248797616239303,\n",
       "  0.02621151145647332,\n",
       "  0.02621184953711232,\n",
       "  0.026578804444996013,\n",
       "  0.02623666684992992,\n",
       "  0.02642222534227642,\n",
       "  0.026576270409357422,\n",
       "  0.0265000064626324,\n",
       "  0.026736527978165402,\n",
       "  0.026346350727483498,\n",
       "  0.026637774828673376,\n",
       "  0.026454027371975046,\n",
       "  0.026531955760760898,\n",
       "  0.026747896878195312,\n",
       "  0.026731045928701385,\n",
       "  0.02669287602300095,\n",
       "  0.02684126280810131,\n",
       "  0.02666796491287244,\n",
       "  0.026515239406579506,\n",
       "  0.026534863548943983,\n",
       "  0.026946548056235087,\n",
       "  0.026936538735735444,\n",
       "  0.02703452230756687,\n",
       "  0.026875035312247296,\n",
       "  0.02703681888178124,\n",
       "  0.026664124929315174,\n",
       "  0.027025758551081698],\n",
       " [0.21174371451743715,\n",
       "  0.36982968369829683,\n",
       "  0.4315652879156529,\n",
       "  0.4678345498783455,\n",
       "  0.4967396593673966,\n",
       "  0.5162692619626926,\n",
       "  0.5263909164639091,\n",
       "  0.5425790754257908,\n",
       "  0.5505920519059205,\n",
       "  0.5536415247364153,\n",
       "  0.5648986212489863,\n",
       "  0.5655798864557988,\n",
       "  0.56992700729927,\n",
       "  0.5745336577453366,\n",
       "  0.5813138686131387,\n",
       "  0.5844606650446067,\n",
       "  0.5874128142741282,\n",
       "  0.589456609894566,\n",
       "  0.590235198702352,\n",
       "  0.5930251419302515,\n",
       "  0.5924087591240876,\n",
       "  0.5955879967558799,\n",
       "  0.6000973236009732,\n",
       "  0.59779399837794,\n",
       "  0.6011354420113544,\n",
       "  0.6001297648012976,\n",
       "  0.6032441200324412,\n",
       "  0.6041524736415247,\n",
       "  0.6024330900243309,\n",
       "  0.6057096512570965,\n",
       "  0.6031143552311435,\n",
       "  0.604087591240876,\n",
       "  0.6105109489051095,\n",
       "  0.6090510948905109,\n",
       "  0.6077858880778588,\n",
       "  0.6122627737226277,\n",
       "  0.6079156528791565,\n",
       "  0.6098296836982968,\n",
       "  0.6098296836982968,\n",
       "  0.610316301703163,\n",
       "  0.6090186536901865,\n",
       "  0.6133008921330089,\n",
       "  0.6149878345498784,\n",
       "  0.6166098945660989,\n",
       "  0.6160908353609084,\n",
       "  0.615831305758313,\n",
       "  0.6154744525547445,\n",
       "  0.6157339821573399,\n",
       "  0.6132035685320357,\n",
       "  0.6130413625304136,\n",
       "  0.6165774533657745,\n",
       "  0.6169018653690187,\n",
       "  0.6171938361719383,\n",
       "  0.6169018653690187,\n",
       "  0.6182968369829683,\n",
       "  0.6143390105433901,\n",
       "  0.6148256285482563,\n",
       "  0.6178102189781022,\n",
       "  0.6175182481751825,\n",
       "  0.6158961881589619,\n",
       "  0.6181670721816708,\n",
       "  0.6198540145985402,\n",
       "  0.6177128953771289,\n",
       "  0.6201135442011354,\n",
       "  0.6161557177615572,\n",
       "  0.6192700729927008,\n",
       "  0.6209894566098946,\n",
       "  0.6193998377939984,\n",
       "  0.6212165450121655,\n",
       "  0.6220275750202757,\n",
       "  0.6214436334144363,\n",
       "  0.6191403081914031,\n",
       "  0.6192700729927008,\n",
       "  0.623941605839416,\n",
       "  0.6185888077858881,\n",
       "  0.6171613949716139,\n",
       "  0.6143390105433901,\n",
       "  0.6189456609894566,\n",
       "  0.6241362530413626,\n",
       "  0.6198864557988646,\n",
       "  0.623227899432279,\n",
       "  0.6217680454176805,\n",
       "  0.6266666666666667,\n",
       "  0.6203081914030819,\n",
       "  0.6250121654501216,\n",
       "  0.6207948094079481,\n",
       "  0.6190105433901054,\n",
       "  0.6230332522303326,\n",
       "  0.6247850770478508,\n",
       "  0.6227088402270884,\n",
       "  0.6223844282238443,\n",
       "  0.6238118410381184,\n",
       "  0.6220600162206001,\n",
       "  0.6205028386050284,\n",
       "  0.6235523114355231,\n",
       "  0.6224817518248175,\n",
       "  0.6219951338199513,\n",
       "  0.6215085158150851,\n",
       "  0.6271208434712084,\n",
       "  0.6214760746147607])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_model(net3,\n",
    "               train_data=X_train_PCA,train_labels=y_train, \n",
    "                test_data = X_test_PCA, test_labels = y_test,\n",
    "                epochs=100, batch_size = 128,\n",
    "                optimizer=optimizer3,\n",
    "            criterion=criterion ,\n",
    "                train=True,\n",
    "                shuffle=True \n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 82.3 ms, total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on test data ... \n",
      "Accuracy :  0.8730251419302514\n"
     ]
    }
   ],
   "source": [
    "print (\"Predict on test data ... \")\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"\n",
    "    Prints features with the highest coefficient values, per class \\n\n",
    "    https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_pie: streusel, dutch, peel, fill, core, lattic, cinnamon, crust, pie, appl\n",
      "baby_back_ribs: bast, bone, tender, membran, babyback, slab, rack, back, rib, babi\n",
      "baklava: turkish, syrup, diamond, honey, nut, pistachio, walnut, filo, phyllo, baklava\n",
      "beef_carpaccio: parmesan, hacker, freezer, tenderloin, arugula, thin, shave, thinli, beef, carpaccio\n",
      "beef_tartare: truffl, eesti, raw, tabasco, quail, bunsjpg, yolk, caper, beef, tartar\n",
      "beet_salad: walnut, root, roast, salad, vinegar, vinaigrett, dress, goat, peel, beet\n",
      "beignets: squar, dough, deepfri, evapor, confection, mond, yeast, mardi, orlean, beignet\n",
      "bibimbap: spinach, bap, dab, zucchini, sprout, stone, dolsot, korean, gochujang, bibimbap\n",
      "bread_pudding: warm, loaf, dayold, pour, milk, bread, stale, raisin, cube, pud\n",
      "breakfast_burrito: salsa, quotekocom, wrap, sausag, breakfast, tortilla, relat, scrambl, credit, burrito\n",
      "bruschetta: bread, balsam, sourdough, bruschettastyl, brush, rub, basil, toast, baguett, bruschetta\n",
      "caesar_salad: co, toss, dress, worcestershir, parmesan, lettuc, romain, anchovi, crouton, caesar\n",
      "cannoli: fill, marsala, sicilian, pipe, semisweet, pistachio, tube, shell, ricotta, cannoli\n",
      "caprese_salad: vinegar, insalata, heirloom, bocconcini, ball, drizzl, balsam, basil, mozzarella, capres\n",
      "carrot_cake: moist, shred, cake, frost, pineappl, raisin, walnut, cinnamon, grate, carrot\n",
      "ceviche: tostada, refriger, citru, peruvian, cilantro, serrano, marin, acid, lime, cevich\n",
      "cheese_plate: honey, fruit, manchego, brie, cheddar, nut, grape, chees, cracker, plate\n",
      "cheesecake: york, swirl, sour, graham, crack, chees, nobak, crust, springform, cheesecak\n",
      "chicken_curry: piec, past, simmer, turmer, skinless, powder, thigh, coconut, breast, curri\n",
      "chicken_quesadilla: chicken, wedg, flip, rotisseri, jack, fold, shred, breast, tortilla, quesadilla\n",
      "chicken_wings: chicken, sticki, marin, coat, wingett, drummett, buffalo, joint, drumett, wing\n",
      "chocolate_cake: scratch, birthday, moist, ganach, molten, german, cake, chocol, cocoa, flourless\n",
      "chocolate_mousse: heatproof, tort, semisweet, cocoa, peak, fold, melt, whip, dark, mouss\n",
      "churros: bag, spain, mexican, cinnamonsugar, dough, nozzl, spanish, cinnamon, pipe, churro\n",
      "clam_chowder: dice, bacon, littleneck, juic, thyme, celeri, manhattan, england, chowder, clam\n",
      "club_sandwich: toast, breast, deli, spread, bacon, turkey, lettuc, mayonnais, sandwich, club\n",
      "crab_cakes: crumb, patti, breadcrumb, maryland, mayonnais, lump, crabcak, cake, crabmeat, crab\n",
      "creme_brulee: brulé, blowtorch, custard, yolk, ramekin, crème, brûlée, creme, torch, brule\n",
      "croque_madame: mornay, béchamel, dijon, gruyer, monsieur, bechamel, ham, croquemadam, croqu, madam\n",
      "cup_cakes: paperlin, tin, pipe, decor, muffin, cupcakes, buttercream, frost, liner, cupcak\n",
      "deviled_eggs: peel, mash, mustard, hardcook, mayonnais, paprika, lengthwis, hardboil, yolk, devil\n",
      "donuts: dunk, kreme, sufganiyot, yeast, dunkin, cutter, hole, glaze, doughnut, donut\n",
      "dumplings: selfrais, dumplings, biscuit, flour, bisquick, stew, dough, steamer, drop, dumpl\n",
      "edamame: soya, nutti, toss, thaw, pod, succotash, frozen, soybean, shell, edamam\n",
      "eggs_benedict: florentin, blender, split, yolk, canadian, muffin, english, poach, hollandais, benedict\n",
      "escargots: mushroom, franc, rins, shell, shallot, bourguignonn, parsley, helix, snail, escargot\n",
      "falafel: soak, pita, tzatziki, coriand, parsley, chickpea, ball, cumin, patti, falafel\n",
      "filet_mignon: cargo, toothpick, desir, medallion, rare, mediumrar, tenderloin, steak, filet, mignon\n",
      "fish_and_chips: potato, british, malt, haddock, tartar, cod, batter, fillet, fish, chip\n",
      "foie_gras: torchon, luxuri, sear, terrin, goos, duck, lobe, liver, gra, foie\n",
      "french_fries: peel, strip, starch, frenchfri, frite, ketchup, potato, russet, fri, french\n",
      "french_onion_soup: thyme, swiss, crouton, baguett, stock, broth, soup, gruyer, onion, french\n",
      "french_toast: shallow, mapl, cinnamon, milk, stuf, soak, overnight, syrup, french, toast\n",
      "fried_calamari: toss, deepfri, aioli, clean, ring, marinara, fri, tentacl, squid, calamari\n",
      "fried_rice: pineappl, grain, pea, leftov, basmati, scrambl, soy, wok, rice, fri\n",
      "frozen_yogurt: maker, ice, nonfat, freezer, plain, greek, freez, froyo, frozen, yogurt\n",
      "garlic_bread: yeast, garlicki, soften, clove, spread, parsley, cheesi, bread, loaf, garlic\n",
      "gnocchi: sage, boil, shelfstabl, slot, potato, ricer, rope, surfac, float, gnocchi\n",
      "greek_salad: dress, vinegar, orzo, crumbl, pit, kalamata, cucumb, oregano, feta, greek\n",
      "grilled_cheese_sandwich: flip, melt, bread, sourdough, slice, cheddar, gooey, chees, grill, sandwich\n",
      "grilled_salmon: dill, skinsid, easili, brush, plankgril, fish, skin, grill, fillet, salmon\n",
      "guacamole: scoop, pit, lime, ripe, dip, avocado, chunki, mash, guac, guacamol\n",
      "gyoza: japanes, lid, dumpl, evapor, pleat, jiaozi, potstick, wrapper, sticker, gyoza\n",
      "hamburger: ketchup, pound, beef, lean, cheeseburg, bun, burger, ground, patti, hamburg\n",
      "hot_and_sour_soup: stock, cornstarch, soup, tofu, shoot, vinegar, bamboo, hot, hotandsour, sour\n",
      "hot_dog: flickrcom, automobil, sauerkraut, coney, damndeliciousnet, spiralcut, frank, frankfurt, dog, hotdog\n",
      "huevos_rancheros: salsa, mediumlow, egg, runni, mexican, crack, refri, tortilla, huevo, ranchero\n",
      "hummus: paprika, drain, dip, pita, garbanzo, processor, smooth, chickpea, tahini, hummu\n",
      "ice_cream: heavi, oreo, churn, freezer, vanilla, maker, freez, cream, icecream, ice\n",
      "lasagna: cottag, bechamel, noboil, repeat, mozzarella, lasagn, ricotta, noodl, layer, lasagna\n",
      "lobster_bisque: meat, kraft, cognac, claw, stock, brandi, tail, sherri, lobster, bisqu\n",
      "lobster_roll_sandwich: shack, mayonnais, claw, tail, england, pageok, bun, sandwich, roll, lobster\n",
      "macaroni_and_cheese: sharp, mustard, crumb, dent, milk, pasta, cheddar, elbow, mac, macaroni\n",
      "macarons: buttercream, parchment, feet, shell, almond, meringu, ganach, pipe, macaroon, macaron\n",
      "miso_soup: enoki, cube, ladl, dissolv, soup, past, wakam, tofu, dashi, miso\n",
      "mussels: shell, beard, crusti, moul, steam, scrub, discard, open, debeard, mussel\n",
      "nachos: sour, cheddar, kidney, shred, jalapeno, platter, melt, tortilla, chip, nacho\n",
      "omelette: flip, tilt, slide, nonstick, spanish, pour, beat, fold, omelet, omelett\n",
      "onion_rings: dip, crumb, beerbatt, slice, beer, vidalia, batter, separ, onion, ring\n",
      "oysters: aphrodisiac, cornmeal, liquor, curl, dozen, mignonett, shell, rockefel, shuck, oyster\n",
      "pad_thai: toss, wedg, lime, sprout, wok, noodl, peanut, tamarind, thai, pad\n",
      "paella: valenciana, valencia, chorizo, rice, paprika, pea, spain, spanish, saffron, paella\n",
      "pancakes: syrup, flour, buttermilk, fluffi, stack, bubbl, batter, griddl, flip, pancak\n",
      "panna_cotta: invert, mould, unmold, ramekin, unflavor, dissolv, pannacotta, gelatin, panna, cotta\n",
      "peking_duck: hoi, mandarin, beij, carv, cucumb, pancak, skin, hoisin, duck, peke\n",
      "pho: bone, fuh, wedg, noodl, sprout, bo, broth, anis, vietnames, pho\n",
      "pizza: calzon, base, margherita, stone, prebak, pepperoni, mozzarella, dough, crust, pizza\n",
      "pork_chop: appl, brine, porkchop, centercut, loin, thick, boneless, bonein, chop, pork\n",
      "poutine: quebecoi, squeak, montreal, canada, quebec, squeaki, canadian, gravi, curd, poutin\n",
      "prime_rib: christma, thermomet, stand, roast, rare, ju, horseradish, carv, rib, prime\n",
      "pulled_pork_sandwich: fork, slaw, coleslaw, barbecu, butt, shoulder, pork, bun, sandwich, pull\n",
      "ramen: broth, softboil, shoyu, colleg, tonkotsu, packag, inspiritoocom, noodl, packet, ramen\n",
      "ravioli: squar, pasta, marinara, parmesan, wrapper, wonton, seal, fill, cutter, ravioli\n",
      "red_velvet_cake: bottl, redvelvet, frost, southern, buttermilk, cocoa, vinegar, color, red, velvet\n",
      "risotto: barley, broth, carnaroli, ladl, creami, parmesan, rice, absorb, arborio, risotto\n",
      "samosa: garam, knead, fill, fold, chutney, pea, seal, triangl, cone, samosa\n",
      "sashimi: soy, yellowtail, daikon, sushi, matsuhisa, thinli, raw, wasabi, sashimigrad, sashimi\n",
      "scallops: shellfish, overcook, pansear, bay, coral, opaqu, baconwrap, sear, sea, scallop\n",
      "seaweed_salad: vinegar, seed, iodin, hijiki, aram, soak, sesam, salad, wakam, seawe\n",
      "shrimp_and_grits: bacon, quickcook, cheddar, polenta, pink, stoneground, southern, devein, shrimp, grit\n",
      "spaghetti_bolognese: occasion, oregano, celeri, lean, simmer, minc, bol, spag, spaghetti, bolognes\n",
      "spaghetti_carbonara: beaten, grate, dent, guancial, scrambl, parmesan, bacon, pancetta, spaghetti, carbonara\n",
      "spring_rolls: lumpia, tightli, dip, vermicelli, fold, paper, springrol, wrapper, roll, spring\n",
      "steak: marinad, beef, marin, chimichurri, mallet, ribey, skirt, sirloin, flank, steak\n",
      "strawberry_shortcake: cutter, assembl, macer, angel, horizont, biscuit, hull, whip, strawberri, shortcak\n",
      "sushi: wasabi, handrol, shortgrain, avocado, maki, nori, rice, roll, mat, sushi\n",
      "tacos: lime, flake, cumin, shred, soft, corn, tortilla, lettuc, shell, taco\n",
      "takoyaki: stall, batter, tako, japanes, okonomiyaki, osaka, bonito, ball, octopu, takoyaki\n",
      "tiramisu: savoiardi, spong, cocoa, layer, finger, espresso, coffe, ladyfing, mascarpon, tiramisu\n",
      "tuna_tartare: dice, wonton, wasabi, sesam, grade, ahi, mold, sushigrad, tuna, tartar\n",
      "waffles: buttermilk, flour, toaster, syrup, eggo, batter, maker, belgian, iron, waffl\n"
     ]
    }
   ],
   "source": [
    "print_top10(tfidf, clf, all_texts.target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
